{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Basque:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/kepa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /home/kepa/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_tokenizer.pt', 'lang': 'eu', 'shorthand': 'eu_bdt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_tagger.pt', 'pretrain_path': '/home/kepa/eu/eu_bdt_models/eu_bdt.pretrain.pt', 'lang': 'eu', 'shorthand': 'eu_bdt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_lemmatizer.pt', 'lang': 'eu', 'shorthand': 'eu_bdt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_parser.pt', 'pretrain_path': '/home/kepa/eu/eu_bdt_models/eu_bdt.pretrain.pt', 'lang': 'eu', 'shorthand': 'eu_bdt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tFruta\tFruta\tNOUN\t_\tCase=Abs|Definite=Def\t0\troot\tfruit cooked in sugar syrup and encrusted with a sugar crystals\tFruta:{'fruta_konfitatu'}\thttps://upload.wikimedia.org/wikipedia/commons/b/b0/Ata_Sugar-apple_Pinha_Fruta_do_conde.JPG\t7600506\n",
      "2\tizozkia\tizozki\tNOUN\t_\tAnimacy=Inan|Case=Abs|Definite=Def|Number=Sing\t3\tobj\ta frozen dessert with fruit flavoring (especially one containing no milk)\t_\t\t7614198\n",
      "3\tegiteko\tegin\tVERB\t_\tCase=Loc\t4\tadvcl\tenter or assume a certain state or condition\tegiteko:{'bihurtu', 'iz.-egin', 'izond.-tu', 'bilakatu'}\t\t149583\n",
      "4\tosagaiak\tosagai\tNOUN\t_\tAnimacy=Inan|Case=Abs|Definite=Def|Number=Plur\t1\tnsubj\ta component of a mixture or compound\t_\t\t3570709\n",
      "5\t:\t:\tPUNCT\t_\t_\t1\tpunct\t\t_\t\t0\n",
      "1\tkilo\tkilo\tNOUN\t_\t_\t20\tnsubj\tthe staff on which wool or flax is wound before spinning\tkilo:{'goru', 'linai'}\thttps://upload.wikimedia.org/wikipedia/commons/9/99/Wiktionary-logo-en-v2.svg\t3212406\n",
      "2\terdi\terdi\tNOUN\t_\t_\t1\tdet\tthe property of being central\t_\thttps://upload.wikimedia.org/wikipedia/commons/9/99/WPanthroponymy.svg\t5078025\n",
      "3\tmarrubi\tmarrubi\tNOUN\t_\tCase=Abs|Definite=Ind\t2\tappos\tsweet fleshy red fruit\t_\thttps://upload.wikimedia.org/wikipedia/commons/a/ae/Fraises_1_Luc_Viatour.jpg\t7745940\n",
      "4\t,\t,\tPUNCT\t_\t_\t3\tpunct\t\t_\t\t0\n",
      "5\tedalontzi\tedalontzi\tNOUN\t_\t_\t6\tnmod\ta vessel intended for drinking\t_\t\t3241496\n",
      "6\terdi\terdi\tNOUN\t_\t_\t7\tnmod\t\t_\t\t0\n",
      "7\tur\tur\tNOUN\t_\t_\t3\tconj\ta facility that provides a source of water\tur:{'ur-hornidura'}\thttps://upload.wikimedia.org/wikipedia/commons/a/a0/20160105-Abraham_house_in_Ur_Iraq.jpg\t4562658\n",
      "8\t,\t,\tPUNCT\t_\t_\t11\tpunct\t\t_\t\t0\n",
      "9\t10\t10\tNUM\t_\tNumType=Card\t10\tnummod\t\t_\t\t0\n",
      "10\tkoilarakada\tkoilarakada\tNOUN\t_\t_\t11\tnmod\tas much as a spoon will hold\t_\t\t13770169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/datos/Dropbox/ikerkuntza/metrix-env/lib/python3.6/site-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /media/datos/Dropbox/ikerkuntza/metrix-env/lib/python3.6/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\tazukre\tazukre\tNOUN\t_\t_\t7\tconj\ta white crystalline carbohydrate used as a sweetener and preservative\t_\t\t7859284\n",
      "12\t,\t,\tPUNCT\t_\t_\t14\tpunct\t\t_\t\t0\n",
      "13\t3\t3\tNUM\t_\tNumType=Card\t14\tnummod\t\t_\t\t0\n",
      "14\tarrautza\tarrautza\tNOUN\t_\t_\t11\tnmod\tanimal reproductive body consisting of an ovum or embryo together with nutritive and protective envelopes; especially the thin-shelled reproductive body laid by e.g. female birds\t_\t\t1460457\n",
      "15\tzuringo\tzurin\tADJ\t_\t_\t14\tamod\t\t_\t\t0\n",
      "16\teta\teta\tCCONJ\t_\t_\t19\tcc\t\t_\t\t0\n",
      "17\tlimoi\tlimoi\tNOUN\t_\t_\t19\tnmod\tyellow oval fruit with juicy acidic flesh\t_\thttps://upload.wikimedia.org/wikipedia/commons/e/e4/Lemon.jpg\t7749582\n",
      "18\tbaten\tbat\tNUM\t_\tNumType=Card\t17\tnummod\t\t_\t\t0\n",
      "19\tzukua\tzuku\tNOUN\t_\tAnimacy=Inan|Case=Abs|Definite=Def|Number=Sing\t15\tconj\tthe liquid part that can be extracted from plant or animal tissue by squeezing or cooking\tzukua:{'ur'}\thttps://upload.wikimedia.org/wikipedia/commons/b/b6/Catatumbolightning.jpg\t7923748\n",
      "20\t.\t.\tPUNCT\t_\t_\t28\tpunct\t\t_\t\t0\n",
      "21\tLehenik\tlehenik\tADV\t_\t_\t25\tadvmod\t\t_\t\t0\n",
      "22\t,\t,\tPUNCT\t_\t_\t28\tpunct\t\t_\t\t0\n",
      "23\tmarrubiak\tmarrubi\tNOUN\t_\tAnimacy=Inan|Case=Abs|Definite=Def|Number=Plur\t28\tobj\tsweet fleshy red fruit\t_\thttps://upload.wikimedia.org/wikipedia/commons/a/ae/Fraises_1_Luc_Viatour.jpg\t7745940\n",
      "24\tondo\tondo\tADV\t_\t_\t25\tadvmod\t\t_\t\t0\n",
      "25\tgarbitu\tgarbitu\tVERB\t_\tVerbForm=Part\t19\tconj\tclean one's body or parts thereof, as by washing\tgarbitu:{'txukundu', 'apaindu'}\t\t35758\n",
      "26\t,\t,\tPUNCT\t_\t_\t28\tpunct\t\t_\t\t0\n",
      "27\tzurtoina\tzurtoin\tNOUN\t_\tCase=Abs|Definite=Def|Number=Sing\t28\tobj\t\t_\t\t0\n",
      "28\tkendu\tkendu\tVERB\t_\tVerbForm=Part\t34\tconj\ttake away\tkendu:{'gabetu'}\t\t172732\n",
      "29\teta\teta\tCCONJ\t_\t_\t30\tcc\t\t_\t\t0\n",
      "30\tzatitu\tzatitu\tVERB\t_\tVerbForm=Part\t28\tconj\tseparate (substances) into constituent elements or parts\tzatitu:{'bereizi', 'banatu'}\t\t209174\n",
      "31\t.\t.\tPUNCT\t_\t_\t34\tpunct\t\t_\t\t0\n",
      "32\tOndoren\tondoren\tADV\t_\t_\t30\tadvmod\t\t_\t\t0\n",
      "33\t,\t,\tPUNCT\t_\t_\t34\tpunct\t\t_\t\t0\n",
      "34\tnahasi\tnahasi\tVERB\t_\tVerbForm=Part\t0\troot\tadd as an additional element or part\tnahasi:{'nahastu'}\t\t184117\n",
      "35\tbeste\tbeste\tDET\t_\t_\t36\tdet\t\t_\t\t0\n",
      "36\tosagai\tosagai\tNOUN\t_\t_\t40\tnsubj\ta component of a mixture or compound\t_\thttps://upload.wikimedia.org/wikipedia/commons/6/6f/Asia_laea_location_map.svg\t3570709\n",
      "37\tguztiak\tguzti\tDET\t_\tCase=Abs|Definite=Def|Number=Plur\t36\tdet\t\t_\t\t0\n",
      "38\tontzi\tontzi\tNOUN\t_\t_\t45\tobl\tany object that can be used to hold things (especially a large metal boxlike object of standardized dimensions that can be loaded from one form of transport to another)\tontzi:{'edukiontzi', 'kontainer'}\t\t3094503\n",
      "39\tbatean\tbat\tNUM\t_\tNumType=Card\t38\tnummod\t\t_\t\t0\n",
      "40\teta\teta\tCCONJ\t_\t_\t41\tcc\t\t_\t\t0\n",
      "41\tirabiatu\tirabiatu\tVERB\t_\tVerbForm=Part\t34\tconj\twhip with or as if with a wire whisk\t_\t\t1417868\n",
      "42\t.\t.\tPUNCT\t_\t_\t45\tpunct\t\t_\t\t0\n",
      "43\tJarraian\tjarraian\tADV\t_\t_\t45\tadvmod\t\t_\t\t0\n",
      "44\t,\t,\tPUNCT\t_\t_\t46\tpunct\t\t_\t\t0\n",
      "45\tgehitu\tgehitu\tVERB\t_\tVerbForm=Part\t40\tconj\tmake bigger or more\tgehitu:{'igo', 'handitu', 'gehiagotu', 'hazi', 'gora_egin', 'handiagotu'}\t\t153263\n",
      "46\tmarrubiak\tmarrubi\tNOUN\t_\tAnimacy=Inan|Case=Abs|Definite=Def|Number=Plur\t51\tnmod\t\t_\t\t0\n",
      "47\teta\teta\tCCONJ\t_\t_\t48\tcc\t\t_\t\t0\n",
      "48\ttxikitu\ttxikitu\tVERB\t_\tVerbForm=Part\t55\tconj\tdestroy or injure severely\ttxikitu:{'deboilatu', 'birrindu', 'hondatu', 'suntsitu'}\t\t292672\n",
      "49\tondo\tondo\tADV\t_\t_\t55\tadvmod\t\t_\t\t0\n",
      "50\tondo\tondo\tADV\t_\t_\t55\tadvmod\t\t_\t\t0\n",
      "51\tirabiagailuaz\tirabiagailu\tNOUN\t_\tCase=Ins|Definite=Def|Number=Sing\t45\tobl\ta mixer for beating eggs or whipping cream\t_\t\t3266371\n",
      "52\t.\t.\tPUNCT\t_\t_\t46\tpunct\t\t_\t\t0\n",
      "53\tGero\tgero\tADV\t_\t_\t55\tadvmod\t\t_\t\t0\n",
      "54\t,\t,\tPUNCT\t_\t_\t55\tpunct\t\t_\t\t0\n",
      "55\testali\testali\tVERB\t_\tVerbForm=Part\t60\tconj\tform a cover over\testali:{'tapatu'}\t\t1207951\n",
      "56\tfilm\tfilm\tNOUN\t_\t_\t52\tobl\ta piece of photographic film showing an image with light and shade or colors reversed\tfilm:{'negatibo'}\thttps://upload.wikimedia.org/wikipedia/commons/a/ad/BolexH16.jpg\t3817191\n",
      "57\tgardenaz\tgarden\tADJ\t_\tCase=Ins|Definite=Def|Number=Sing\t56\tamod\t\t_\t\t0\n",
      "58\teta\teta\tCCONJ\t_\t_\t59\tcc\t\t_\t\t0\n",
      "59\tsartu\tsartu\tVERB\t_\tVerbForm=Part\t56\tconj\tto introduce (a new aspect or element)\tsartu:{'sarrarazi', 'barruratu', 'barneratu'}\t\t187268\n",
      "60\tizozkailuan\tizozkailu\tNOUN\t_\tAnimacy=Inan|Case=Ine|Definite=Def|Number=Sing\t65\tobl\telectric refrigerator (trade name Deepfreeze) in which food is frozen and stored for long periods of time\t_\t\t3170635\n",
      "61\t.\t.\tPUNCT\t_\t_\t69\tpunct\t\t_\t\t0\n",
      "62\tOrdu\tordu\tNOUN\t_\t_\t69\tobl\tdistance measured by the time taken to cover it\tOrdu:{'minutu'}\thttps://upload.wikimedia.org/wikipedia/commons/d/dd/Flag_of_Azerbaijan.svg\t5131023\n",
      "63\teta\teta\tCCONJ\t_\t_\t64\tcc\t\t_\t\t0\n",
      "64\terdi\terdi\tADV\t_\t_\t62\tconj\t\t_\t\t0\n",
      "65\tigaro\tigaro\tVERB\t_\tVerbForm=Part\t59\tconj\tmove past\tigaro:{'pasatu', 'joan', 'iragan'}\t\t2051694\n",
      "66\tondoren\tondoren\tADV\t_\t_\t65\tadvcl\t\t_\t\t0\n",
      "67\t,\t,\tPUNCT\t_\t_\t72\tpunct\t\t_\t\t0\n",
      "68\tizozkailutik\tizozkailu\tNOUN\t_\tAnimacy=Inan|Case=Abl|Definite=Def|Number=Sing\t69\tobl\telectric refrigerator (trade name Deepfreeze) in which food is frozen and stored for long periods of time\t_\t\t3170635\n",
      "69\tatera\tatera\tVERB\t_\tVerbForm=Part\t74\tadvcl\tremove something concrete, as by lifting, pushing, or taking off, or remove something abstract\tatera:{'hartu', 'kendu', 'eraman'}\t\t173338\n",
      "70\teta\teta\tCCONJ\t_\t_\t72\tcc\t\t_\t\t0\n",
      "71\tberriro\tberriro\tADV\t_\t_\t72\tadvmod\t\t_\t\t0\n",
      "72\tirabiatu\tirabiatu\tVERB\t_\tVerbForm=Part\t69\tconj\t\t_\t\t0\n",
      "73\t.\t.\tPUNCT\t_\t_\t75\tpunct\t\t_\t\t0\n",
      "74\tBanatu\tbanatu\tVERB\t_\tVerbForm=Part\t75\tadvcl\tseparate (substances) into constituent elements or parts\tBanatu:{'zatitu', 'bereizi'}\t\t209174\n",
      "75\tontzietan\tontzi\tNOUN\t_\tAnimacy=Inan|Case=Ine|Definite=Def|Number=Plur\t55\tobl\tany object that can be used to hold things (especially a large metal boxlike object of standardized dimensions that can be loaded from one form of transport to another)\tontzietan:{'edukiontzi', 'kontainer'}\t\t3094503\n",
      "76\teta\teta\tCCONJ\t_\t_\t77\tcc\t\t_\t\t0\n",
      "77\tsartu\tsartu\tVERB\t_\tVerbForm=Part\t69\tconj\t\t_\t\t0\n",
      "78\tberriro\tberriro\tADV\t_\t_\t77\tadvmod\t\t_\t\t0\n",
      "79\tizozkailuan\tizozkailu\tNOUN\t_\tAnimacy=Inan|Case=Ine|Definite=Def|Number=Sing\t77\tobl\t\t_\t\t0\n",
      "80\tordubetez\tordubete\tNOUN\t_\tAnimacy=Inan|Case=Ins|Definite=Ind\t77\tobl\t\t_\t\t0\n",
      "81\t.\t.\tPUNCT\t_\t_\t84\tpunct\t\t_\t\t0\n",
      "82\tAzkenik\tazkenik\tADV\t_\t_\t84\tadvmod\t\t_\t\t0\n",
      "83\t,\t,\tPUNCT\t_\t_\t84\tpunct\t\t_\t\t0\n",
      "84\tatera\tatera\tVERB\t_\tVerbForm=Part\t85\tadvcl\t\t_\t\t0\n",
      "85\tizozkailutik\tizozkailu\tNOUN\t_\tAnimacy=Inan|Case=Abl|Definite=Def|Number=Sing\t77\tobl\t\t_\t\t0\n",
      "86\teta\teta\tCCONJ\t_\t_\t87\tcc\t\t_\t\t0\n",
      "87\tjan\tjan\tVERB\t_\tVerbForm=Part\t85\tconj\tcause to deteriorate due to the action of water, air, or an acid\t_\t\t274283\n",
      "88\t.\t.\tPUNCT\t_\t_\t69\tpunct\t\t_\t\t0\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import nltk\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "\n",
    "##################WORDFRECUENCY###################################################\n",
    "# load a freq file into a dictionary\n",
    "import codecs\n",
    "d = {}\n",
    "#f = codecs.open('compounds.dat', encoding='utf-8')\n",
    "with codecs.open('LB2014Maiztasunak_zenbakiakKenduta.csv',encoding='utf-8') as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "       (val,key) = line.split(\",\")\n",
    "       d[key] = val\n",
    "#for key,val in d.items():\n",
    " #   print (key, \"=>\", val)\n",
    "    \n",
    "def zipf_frequency_eu(lemma):\n",
    "    if d.get(lemma):\n",
    "        return (int(d.get(lemma)))\n",
    "    else:\n",
    "        return 1\n",
    "#################WORDNET##########################\n",
    "#Instalar 'wordnet' \n",
    "nltk.download('wordnet')\n",
    "#Add multilingual wordnet\n",
    "nltk.download('omw')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def set_is_empty(some_set):\n",
    "    return some_set == set()\n",
    "\n",
    "###################WSD####################################################\n",
    "#simple_lesk erabiltzeko\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#from pywsd.lesk import simple_lesk\n",
    "from nltk.wsd import lesk\n",
    "###############ImageNet###########################\n",
    "#from bs4 import BeautifulSoup\n",
    "#import requests\n",
    "#def imagenet_offset2url(offset):\n",
    "#    soffset=str(offset).zfill(8)\n",
    "#    query=\"http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n\"+str(soffset)\n",
    "#    page = requests.get(query)#ship synset\n",
    "    # BeautifulSoup is an HTML parsing library\n",
    "    #puts the content of the website into the soup variable, each url on a different line\n",
    "#    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#    str_soup=str(soup)#convert soup to string so it can be split\n",
    "#    split_urls=str_soup.split('\\r\\n')#split so each url is a different possition on a list\n",
    "#    return(split_urls[0])\n",
    "###############wikipedia############################\n",
    "import wikipedia\n",
    "def wikipedia_offset2url(text):\n",
    "#WIKIPEDIA\n",
    "    url= ''\n",
    "    ext = ''\n",
    "    svg = False\n",
    "    i = 0\n",
    "    while (svg==False):\n",
    "        try:\n",
    "            #First of all wikipedia.page() will store all the relevant informations \n",
    "            #from the requested page in the variable imagepage. \n",
    "            imagepage=wikipedia.page(text)\n",
    "            #Existen muchas funciones. imagepage.url devuelve la url, imagepage.url.content el contenido, title el título....\n",
    "            #imagepage.images[0] will return the URL of the image that is present at index 0. \n",
    "            #If you want to fetch another image use index as 1, 2, 3, etc, according to images present in the page.\n",
    "            url = imagepage.images[i]\n",
    "            #print(url)\n",
    "            name, ext =(url.split('/')[-1].split('.'))\n",
    "            if ext=='.svg':\n",
    "                  i = i+1\n",
    "            else:\n",
    "                  svg = True\n",
    "        except:\n",
    "            url= ''\n",
    "            svg = True\n",
    "    return url\n",
    "                #si no desambiguamos\n",
    "##########Wikidata###################################\n",
    "#Wikidata query service -> code in python\n",
    "#pip install wikidata\n",
    "#https://pypi.org/project/Wikidata/\n",
    "# pip install sparqlwrapper\n",
    "# https://rdflib.github.io/sparqlwrapper/\n",
    "import wikidata\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from wikidata.client import Client\n",
    "def get_results(endpoint_url, query):\n",
    "    sparql = SPARQLWrapper(endpoint_url)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "def wikidata_offset2url(offset):\n",
    "    #WIKIDATA\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    soffset=str(offset).zfill(8)\n",
    "    query=\"Select * where {?item wdt:P2888 <http://wordnet-rdf.princeton.edu/wn30/\"+soffset+\"-n> }\"\n",
    "    try:\n",
    "        results = get_results(endpoint_url, query)\n",
    "        ema=results[\"results\"][\"bindings\"]\n",
    "        item=ema[0].get('item')\n",
    "        value=item.get('value')\n",
    "        head, tail = value.split(\"http://www.wikidata.org/entity/\")\n",
    "        #for result in results[\"results\"][\"bindings\"]:\n",
    "          #    print(result)\n",
    "        client = Client()\n",
    "        entity = client.get(tail, load=True)\n",
    "        image_prop = client.get('P18')\n",
    "        image = entity[image_prop]\n",
    "        #image-><wikidata.commonsmedia.File 'File:KBS \"The Producers\" press conference, 11 May 2015 10.jpg'>\n",
    "        #image.image_resolution\n",
    "        url=image.image_url\n",
    "    except:\n",
    "        url=''\n",
    "    return url\n",
    "#####################################################\n",
    "##############STOPWORDS################################################################################\n",
    "#Generating a word cloud with no optional parameters based on the above string:\n",
    "#from wordcloud import STOPWORDS\n",
    "#stopwords = set(STOPWORDS)\n",
    "#from nltk.corpus import stopwords\n",
    "#stopwords = set(stopwords.words(\"english\"))\n",
    "#words = [w for w in words if not w in stopwords]\n",
    "#stopwords=set()\n",
    "#with open(\"stopwords_formak.txt\") as file:\n",
    "#    for line in file:\n",
    "#        line = line.strip() #preprocess line\n",
    "#        stopwords.add(line)\n",
    "#print (stopwords)\n",
    "stopwords= set(line.strip() for line in open('stopwords_formaketakonektoreak.txt'))\n",
    "stopwords.add(\"ondo\")\n",
    "stopwords.add(\"ordu\")\n",
    "stopwords.add(\"jarraian\")\n",
    "stopwords.add(\"igaro\")\n",
    "#Gehitu konektoreak\n",
    "#stopwords={'to', 'of', 'us'}\n",
    "\n",
    "\n",
    "###################PARSER#################################################\n",
    "import stanfordnlp #->/media/datos/Dropbox/ikerkuntza/metrix-env/lib/python3.6/site-packages/stanfordnlp\n",
    "MODELS_DIR = '/home/kepa/eu'\n",
    "#stanfordnlp.download('eu', MODELS_DIR) # Download the English models\n",
    "config = {'processors': 'tokenize,pos,lemma,depparse', # Comma-separated list of processors to use\n",
    "'lang': 'eu', # Language code for the language to build the Pipeline in\n",
    "'tokenize_model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_tokenizer.pt', # Processor-specific arguments are set with keys \"{processor_name}_{argument_name}\"\n",
    "'pos_model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_tagger.pt',\n",
    "'pos_pretrain_path': '/home/kepa/eu/eu_bdt_models/eu_bdt.pretrain.pt',\n",
    "'lemma_model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_lemmatizer.pt',\n",
    "'depparse_model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_parser.pt',\n",
    "'depparse_pretrain_path': '/home/kepa/eu/eu_bdt_models/eu_bdt.pretrain.pt'\n",
    "}\n",
    "parser = stanfordnlp.Pipeline(**config)\n",
    "####input file##################\n",
    "input=\"errezeta.txt\" #sys.argv[1]\n",
    "\n",
    "####nivel de dificultad#########\n",
    "difficult = 100000 #int(sys.argv[2])\n",
    "\n",
    "####ouput files#################\n",
    "#estadisticos\n",
    "estadisticaoutput=input+\".out.csv\"\n",
    "\n",
    "#Write all the information in the file\n",
    "estfile = open(estadisticaoutput, \"w\")\n",
    "\n",
    "\n",
    "###############Tratamiento de texto###############################################\n",
    "#quitar todos los retornos \\n si contiene\n",
    "text = open(input).read().replace('\\n', '')\n",
    "#remove text inside parentheses\n",
    "#text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "#separa , . ! ( ) ? ; del texto con espacios, teniendo en cuenta que los no son numeros en el caso de , y . \n",
    "text = re.sub(r\"\\_\", \" \", text)\n",
    "text = re.sub(r\"\\-\", \" \", text)\n",
    "text = re.sub(r'[.]+(?![0-9])', r' . ', text)\n",
    "text = re.sub(r'[,]+(?![0-9])', r' , ', text)\n",
    "text = re.sub(r\"!\", \" ! \", text)\n",
    "text = re.sub(r\"\\(\", \" ( \", text)\n",
    "text = re.sub(r\"\\)\", \" ) \", text)\n",
    "text = re.sub(r\"\\?\", \" ? \", text)\n",
    "text = re.sub(r\";\", \" ; \", text)\n",
    "#sustituye 2 espacios seguidos por 1\n",
    "text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "################WORDCLOUD################################################\n",
    "from wordcloud import WordCloud\n",
    "#Generating a word cloud with no optional parameters based on the above string:\n",
    "# #This is because the wordcloud module ignores stopwords by default. Refer to Part 1 of the NLTK tutorial if the concept of stopwords is new to you.If we wish, we can specify our own set of stopwords, instead of the stopwords provided by default.\n",
    "# #Con relative_scaling = 0, solo se consideran los rangos de las palabras. Si modificamos esto a relative_scaling = 1.0, entonces una palabra que aparece dos veces más frecuentemente aparecerá dos veces el tamaño. Por defecto, relative_scaling = 0.5.\n",
    "# wordcloud = WordCloud(relative_scaling=1.0, stopwords={'to', 'of'}).generate(text)\n",
    "wordcloud = WordCloud(relative_scaling=1.0,stopwords=stopwords).generate(text)\n",
    "#Finally, use matplotlib to render the word cloud:\n",
    "#plot_wordcloud(wordcloud)\n",
    "wordcloudfilename=input+\".png\"\n",
    "#wordcloudfilename=\"resume.png\"\n",
    "wordcloud.to_file(wordcloudfilename)\n",
    "###################analizador morfosintactico#################################################\n",
    "palabras_diferentes = []\n",
    "doc=parser(text)\n",
    "for sent in doc.sentences:\n",
    "    #Por cada sentencia\n",
    "    for entry in sent.words:\n",
    "        definition = ''\n",
    "        synonyms = []\n",
    "        offset = 0\n",
    "        nueve = '_' \n",
    "        url = ''              \n",
    "        #Por cada palabra\n",
    "        if entry.text.isalpha() and (entry.upos == 'NOUN' or entry.upos == 'VERB') and entry.text.lower() not in palabras_diferentes:\n",
    "            #Si la palabra es un content word\n",
    "            palabras_diferentes.append(entry.text.lower())\n",
    "            lemafrequency = zipf_frequency_eu(entry.lemma)\n",
    "            #<=6 zaila\n",
    "            #<=34 erdiko zailtasuna\n",
    "            #<=100000 errazak\n",
    "            if lemafrequency <= int(difficult):\n",
    "                lema=entry.lemma\n",
    "                synset_ids = wn.synsets(lema,lang='eus')\n",
    "                if entry.upos=='NOUN':\n",
    "                    patron='n'\n",
    "                if entry.upos=='VERB':\n",
    "                    patron='v'\n",
    "                #intentamos obtener la synset desambiguada, y la def,exam,sin and img-url sobre esa synset\n",
    "                try:\n",
    "                    synset_desambiguado = lesk(text.split(), entry.text, pos=patron, synsets=synset_ids)\n",
    "                    #definicion\n",
    "                    definition = synset_desambiguado.definition()\n",
    "                    #ejemplos\n",
    "                    examples = synset_desambiguado.examples()\n",
    "                    #offset\n",
    "                    offset = synset_desambiguado.offset()\n",
    "                    #traduccion es\n",
    "                    traduccion_es=answer2.lemmas('spa')\n",
    "                    es_list=[]\n",
    "                    for l in traduccion_es:\n",
    "                        es_list.append(l.name())\n",
    "                    #traduccion eu\n",
    "                    traduccion_eu=answer2.lemmas('eus') \n",
    "                    eu_list=[]\n",
    "                    for l in traduccion_eu:\n",
    "                        eu_list.append(l.name())\n",
    "                    #sinonimos\n",
    "                    for l in synset_desambiguado.lemma_names():\n",
    "                        synonyms.append(l.lower())\n",
    "                    try:\n",
    "                        synonyms.remove(entry.text.lower())\n",
    "                    except:\n",
    "                        pass\n",
    "                    try:\n",
    "                        synonyms.remove(entry.lemma)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    #url imagen\n",
    "                    if entry.upos=='NOUN':\n",
    "                        #deia wikidata\n",
    "                        url=wikidata_offset2url(offset)\n",
    "                        if url == '':\n",
    "                            url=wikipedia_offset2url(entry.text)\n",
    "                #si no desambiguamos#PRIMER SYNSET FILTRADO POR CATEGORIA\n",
    "                except:\n",
    "                    count = 0\n",
    "                    for synset in synset_ids:\n",
    "                        if entry.upos=='NOUN':\n",
    "                            patron='.n.'\n",
    "                        if entry.upos=='VERB':\n",
    "                            patron='.v.'\n",
    "\n",
    "                        if patron in synset.name() and count == 0:\n",
    "                            count = 1\n",
    "                            for l in synset.lemmas(lang='eus'):\n",
    "                                synonyms.append(l.name())\n",
    "\n",
    "                            try:\n",
    "                                synonyms.remove(entry.text.lower())\n",
    "                            except:\n",
    "                                pass\n",
    "                            try:\n",
    "                                synonyms.remove(entry.lemma)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                            definition = synset.definition()\n",
    "                            offset = synset.offset()\n",
    "                            #ejemplos\n",
    "                            examples = synset.examples()\n",
    "                            #traduccion es\n",
    "                            traduccion_es=synset.lemmas('spa')\n",
    "                            es_list=[]\n",
    "                            for l in traduccion_es:\n",
    "                                es_list.append(l.name())\n",
    "                            #traduccion eu\n",
    "                            traduccion_eu=synset.lemmas('eus') \n",
    "                            eu_list=[]\n",
    "                            for l in traduccion_eu:\n",
    "                                eu_list.append(l.name())\n",
    "                            #url imagen\n",
    "                            if entry.upos=='NOUN':\n",
    "                                #deia wikidata\n",
    "                                url=wikidata_offset2url(offset)\n",
    "                                if url == '':\n",
    "                                    url=wikipedia_offset2url(entry.text)\n",
    "\n",
    "        if not set_is_empty(set(synonyms)):\n",
    "            nueve=entry.text+\":\"+str(set(synonyms))\n",
    "        else:\n",
    "            nueve=\"_\"\n",
    "        \n",
    "        print(str(entry.index)+\"\\t\"+entry.text+\"\\t\"+entry.lemma+\"\\t\"+entry.upos+\"\\t\"+entry.xpos+\"\\t\"+entry.feats+\"\\t\"+str(entry.governor)+\"\\t\"+str(entry.dependency_relation)+\"\\t\"+definition+\"\\t\"+nueve+\"\\t\"+str(url)+\"\\t\"+str(offset))\n",
    "        estfile.write(\"%s\" % str(entry.index)+\"\\t\"+entry.text+\"\\t\"+entry.lemma+\"\\t\"+entry.upos+\"\\t\"+entry.xpos+\"\\t\"+entry.feats+\"\\t\"+str(entry.governor)+\"\\t\"+str(entry.dependency_relation[:2])+\"\\t\"+definition+\"\\t\"+nueve+\"\\t\"+str(url)+\"\\t\"+str(offset))\n",
    "        estfile.write(\"\\n\")\n",
    "    estfile.write(\"\\n\")\n",
    "estfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For english:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/kepa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /home/kepa/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/kepa/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/kepa/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/kepa/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/kepa/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/kepa/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/home/kepa/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tFry\tfry\tVERB\tVB\tMood=Imp|VerbForm=Fin\t0\troot\tbe excessively hot\t_\t\t2738241\n",
      "2\tthe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t3\tdet\t\t_\t\t0\n",
      "3\tgarlic\tgarlic\tNOUN\tNN\tNumber=Sing\t1\tobj\tbulbous herb of southern Europe widely naturalized; bulb breaks up into separate strong-flavored cloves\tgarlic:{'Allium_sativum'}\thttps://upload.wikimedia.org/wikipedia/commons/8/8b/All_Garlic_Ail_Ajo.jpg\t12434775\n",
      "4\tand\tand\tCCONJ\tCC\t_\t6\tcc\t\t_\t\t0\n",
      "5\tthe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t6\tdet\t\t_\t\t0\n",
      "6\tchilly\tchilly\tADJ\tJJ\tDegree=Pos\t3\tconj\t\t_\t\t0\n",
      "7\tin\tin\tADP\tIN\t_\t9\tcase\t\t_\t\t0\n",
      "8\tthe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t9\tdet\t\t_\t\t0\n",
      "9\toil\toil\tNOUN\tNN\tNumber=Sing\t6\tobl\t\t_\t\t0\n",
      "10\ton\ton\tADP\tIN\t_\t13\tcase\t\t_\t\t0\n",
      "11\ta\ta\tDET\tDT\tDefinite=Ind|PronType=Art\t13\tdet\t\t_\t\t0\n",
      "12\tmoderate\tmoderate\tADJ\tJJ\tDegree=Pos\t13\tamod\t\t_\t\t0\n",
      "13\theat\theat\tNOUN\tNN\tNumber=Sing\t1\tobl\ta form of energy that is transferred by a difference in temperature\theat:{'heat_energy'}\thttps://upload.wikimedia.org/wikipedia/commons/d/da/171879main_LimbFlareJan12_lg.jpg\t11466043\n",
      "14\t.\t.\tPUNCT\t.\t_\t1\tpunct\t\t_\t\t0\n",
      "1\tAdd\tadd\tVERB\tVB\tMood=Imp|VerbForm=Fin\t0\troot\t\t_\t\t0\n",
      "2\tthe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t4\tdet\t\t_\t\t0\n",
      "3\tbanana\tbanana\tNOUN\tNN\tNumber=Sing\t4\tcompound\tany of several tropical and subtropical treelike herbs of the genus Musa having a terminal crown of large entire leaves and usually bearing hanging clusters of elongated fruits\tbanana:{'banana_tree'}\thttps://upload.wikimedia.org/wikipedia/commons/d/df/2018_06_TropicalIslands_IMG_2170.jpg\t12352287\n",
      "4\tsauce\tsauce\tNOUN\tNN\tNumber=Sing\t1\tobj\tflavorful relish or dressing or topping served as an accompaniment to food\t_\thttps://upload.wikimedia.org/wikipedia/commons/f/fa/Applesauce.jpg\t7829412\n",
      "5\t.\t.\tPUNCT\t.\t_\t1\tpunct\t\t_\t\t0\n",
      "1\tCook\tCook\tVERB\tVB\tMood=Imp|VerbForm=Fin\t0\troot\tprepare a hot meal\t_\t\t1665638\n",
      "2\tthe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t3\tdet\t\t_\t\t0\n",
      "3\tsauce\tsauce\tNOUN\tNN\tNumber=Sing\t1\tobj\t\t_\t\t0\n",
      "4\tfor\tfor\tADP\tIN\t_\t7\tcase\t\t_\t\t0\n",
      "5\tabout\tabout\tADV\tRB\t_\t6\tadvmod\t\t_\t\t0\n",
      "6\t15\t15\tNUM\tCD\tNumType=Card\t7\tnummod\t\t_\t\t0\n",
      "7\tminutes\tminute\tNOUN\tNNS\tNumber=Plur\t1\tobl\t\t_\t\t0\n",
      "8\t.\t.\tPUNCT\t.\t_\t1\tpunct\t\t_\t\t0\n",
      "1\tCook\tCook\tVERB\tVB\tMood=Imp|VerbForm=Fin\t0\troot\t\t_\t\t0\n",
      "2\tthe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t3\tdet\t\t_\t\t0\n",
      "3\tpasta\tpasta\tNOUN\tNN\tNumber=Sing\t1\tobj\ta dish that contains pasta as its main ingredient\t_\t\t7863374\n",
      "4\tin\tin\tSCONJ\tIN\t_\t5\tmark\t\t_\t\t0\n",
      "5\tboiling\tboiling\tVERB\tVBG\tVerbForm=Ger\t1\tadvcl\tcome to the boiling point and change from a liquid to vapor\tboiling:{'boil'}\t\t375021\n",
      "6\twater\twater\tNOUN\tNN\tNumber=Sing\t5\tobj\t\t_\t\t0\n",
      "7\t.\t.\tPUNCT\t.\t_\t1\tpunct\t\t_\t\t0\n",
      "1\tStrain\tStrain\tVERB\tVB\tMood=Imp|VerbForm=Fin\t0\troot\tto exert much effort or energy\tStrain:{'reach', 'strive'}\t\t1146051\n",
      "2\tthe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t3\tdet\t\t_\t\t0\n",
      "3\tpasta\tpasta\tNOUN\tNN\tNumber=Sing\t1\tobj\t\t_\t\t0\n",
      "4\tthrough\tthrough\tADV\tRB\t_\t1\tcompound:prt\t\t_\t\t0\n",
      "5\tand\tand\tCCONJ\tCC\t_\t6\tcc\t\t_\t\t0\n",
      "6\tmix\tmix\tVERB\tVB\tMood=Imp|VerbForm=Fin\t1\tconj\tmix together different elements\tmix:{'immix', 'conflate', 'commingle', 'merge', 'flux', 'meld', 'blend', 'coalesce', 'combine', 'fuse'}\t\t394813\n",
      "7\twell\twell\tADV\tRB\tDegree=Pos\t6\tadvmod\t\t_\t\t0\n",
      "8\twith\twith\tADP\tIN\t_\t10\tcase\t\t_\t\t0\n",
      "9\tthe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t10\tdet\t\t_\t\t0\n",
      "10\tsauce\tsauce\tNOUN\tNN\tNumber=Sing\t6\tobl\t\t_\t\t0\n",
      "11\t.\t.\tPUNCT\t.\t_\t1\tpunct\t\t_\t\t0\n",
      "1\tServe\tserve\tVERB\tVB\tMood=Imp|VerbForm=Fin\t0\troot\tserve a purpose, role, or function\tServe:{'function'}\t\t2670890\n",
      "2\tthe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t3\tdet\t\t_\t\t0\n",
      "3\tpasta\tpasta\tNOUN\tNN\tNumber=Sing\t1\tobj\t\t_\t\t0\n",
      "4\tin\tin\tADP\tIN\t_\t5\tcase\t\t_\t\t0\n",
      "5\tbowls\tbowl\tNOUN\tNNS\tNumber=Plur\t1\tobl\ta round vessel that is open at the top; used chiefly for holding food or liquids\t_\thttps://upload.wikimedia.org/wikipedia/commons/7/75/Bowls%26Kitty.JPG\t2881193\n",
      "6\twith\twith\tADP\tIN\t_\t8\tcase\t\t_\t\t0\n",
      "7\tthe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t8\tdet\t\t_\t\t0\n",
      "8\tparsley\tparsley\tNOUN\tNN\tNumber=Sing\t1\tobl\tannual or perennial herb with aromatic leaves\tparsley:{'Petroselinum_crispum'}\thttps://upload.wikimedia.org/wikipedia/commons/6/62/Flor_de_perejil.JPG\t12942395\n",
      "9\tand\tand\tCCONJ\tCC\t_\t11\tcc\t\t_\t\t0\n",
      "10\tthe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t11\tdet\t\t_\t\t0\n",
      "11\tcheese\tcheese\tNOUN\tNN\tNumber=Sing\t8\tconj\ta solid food prepared from the pressed curd of milk\t_\thttps://upload.wikimedia.org/wikipedia/commons/d/d0/00928_Beskider_K%C3%A4se_aus_Schafsmilch_2013%3B_Sheep%27s-milk_cheeses_from_Poland%3B_Northern_Subcarpathians.JPG\t7850329\n",
      "12\t.\t.\tPUNCT\t.\t_\t1\tpunct\t\t_\t\t0\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "import nltk\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from wordfreq import zipf_frequency\n",
    "\n",
    "#################WORDNET##########################\n",
    "#Instalar 'wordnet' \n",
    "nltk.download('wordnet')\n",
    "#Add multilingual wordnet\n",
    "nltk.download('omw')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def set_is_empty(some_set):\n",
    "    return some_set == set()\n",
    "##################################################\n",
    "\n",
    "from nltk.wsd import lesk\n",
    "###############ImageNet###########################\n",
    "#from bs4 import BeautifulSoup\n",
    "#import requests\n",
    "#def imagenet_offset2url(offset):\n",
    "#    soffset=str(offset).zfill(8)\n",
    "#    query=\"http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n\"+str(soffset)\n",
    "#    page = requests.get(query)#ship synset\n",
    "    # BeautifulSoup is an HTML parsing library\n",
    "    #puts the content of the website into the soup variable, each url on a different line\n",
    "#    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#    str_soup=str(soup)#convert soup to string so it can be split\n",
    "#    split_urls=str_soup.split('\\r\\n')#split so each url is a different possition on a list\n",
    "#    return(split_urls[0])\n",
    "###############wikipedia############################\n",
    "import wikipedia\n",
    "def wikipedia_offset2url(text):\n",
    "#WIKIPEDIA\n",
    "    url= ''\n",
    "    ext = ''\n",
    "    svg = False\n",
    "    i = 0\n",
    "    while (svg==False):\n",
    "        try:\n",
    "            #First of all wikipedia.page() will store all the relevant informations \n",
    "            #from the requested page in the variable imagepage. \n",
    "            imagepage=wikipedia.page(text)\n",
    "            #Existen muchas funciones. imagepage.url devuelve la url, imagepage.url.content el contenido, title el título....\n",
    "            #imagepage.images[0] will return the URL of the image that is present at index 0. \n",
    "            #If you want to fetch another image use index as 1, 2, 3, etc, according to images present in the page.\n",
    "            url = imagepage.images[i]\n",
    "            #print(url)\n",
    "            name, ext =(url.split('/')[-1].split('.'))\n",
    "            if ext=='.svg':\n",
    "                  i = i+1\n",
    "            else:\n",
    "                  svg = True\n",
    "        except:\n",
    "            url= ''\n",
    "            svg = True\n",
    "    return url\n",
    "                #si no desambiguamos\n",
    "##########Wikidata###################################\n",
    "#Wikidata query service -> code in python\n",
    "#pip install wikidata\n",
    "#https://pypi.org/project/Wikidata/\n",
    "# pip install sparqlwrapper\n",
    "# https://rdflib.github.io/sparqlwrapper/\n",
    "import wikidata\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from wikidata.client import Client\n",
    "def get_results(endpoint_url, query):\n",
    "    sparql = SPARQLWrapper(endpoint_url)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "def wikidata_offset2url(offset):\n",
    "    #WIKIDATA\n",
    "    endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "    soffset=str(offset).zfill(8)\n",
    "    query=\"Select * where {?item wdt:P2888 <http://wordnet-rdf.princeton.edu/wn30/\"+soffset+\"-n> }\"\n",
    "    try:\n",
    "        results = get_results(endpoint_url, query)\n",
    "        ema=results[\"results\"][\"bindings\"]\n",
    "        item=ema[0].get('item')\n",
    "        value=item.get('value')\n",
    "        head, tail = value.split(\"http://www.wikidata.org/entity/\")\n",
    "        #for result in results[\"results\"][\"bindings\"]:\n",
    "          #    print(result)\n",
    "        client = Client()\n",
    "        entity = client.get(tail, load=True)\n",
    "        image_prop = client.get('P18')\n",
    "        image = entity[image_prop]\n",
    "        #image-><wikidata.commonsmedia.File 'File:KBS \"The Producers\" press conference, 11 May 2015 10.jpg'>\n",
    "        #image.image_resolution\n",
    "        url=image.image_url\n",
    "    except:\n",
    "        url=''\n",
    "    return url\n",
    "#####################################################\n",
    "#Este método devuelve true en caso de que la palabra pasada como parámetro sea verbo. Para que una palabra sea \n",
    "#verbo se tiene que cumplir que sea VERB o que sea AUX y que su padre NO sea VERB.\n",
    "\n",
    "####analizador sintactico#######################\n",
    "import stanfordnlp\n",
    "stanford = stanfordnlp.Pipeline()\n",
    "#stanfordnlp.download(\"en\")\n",
    "\n",
    "####input file##################\n",
    "input=\"receta.txt\" #sys.argv[1]\n",
    "\n",
    "####nivel de dificultad#########\n",
    "difficult = 5 #int(sys.argv[2])\n",
    "\n",
    "####ouput files#################\n",
    "#estadisticos\n",
    "estadisticaoutput=input+\".out.csv\"\n",
    "\n",
    "#Write all the information in the file\n",
    "estfile = open(estadisticaoutput, \"w\")\n",
    "\n",
    "\n",
    "###############Tratamiento de texto###############################################\n",
    "#quitar todos los retornos \\n si contiene\n",
    "text = open(input).read().replace('\\n', '')\n",
    "#remove text inside parentheses\n",
    "#text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "#separa , . ! ( ) ? ; del texto con espacios, teniendo en cuenta que los no son numeros en el caso de , y . \n",
    "text = re.sub(r'[.]+(?![0-9])', r' . ', text)\n",
    "text = re.sub(r'[,]+(?![0-9])', r' , ', text)\n",
    "text = re.sub(r\"!\", \" ! \", text)\n",
    "text = re.sub(r\"\\(\", \" ( \", text)\n",
    "text = re.sub(r\"\\)\", \" ) \", text)\n",
    "text = re.sub(r\"\\?\", \" ? \", text)\n",
    "text = re.sub(r\";\", \" ; \", text)\n",
    "#sustituye 2 espacios seguidos por 1\n",
    "text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "##############################################################################################\n",
    "#Generating a word cloud with no optional parameters based on the above string:\n",
    "from wordcloud import STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "#stopwords.add(\"every\")\n",
    "#stopwords.add(\"will\")\n",
    "#stopwords={'to', 'of', 'us'}\n",
    "#Generating a word cloud with no optional parameters based on the above string:\n",
    "# #This is because the wordcloud module ignores stopwords by default. Refer to Part 1 of the NLTK tutorial if the concept of stopwords is new to you.If we wish, we can specify our own set of stopwords, instead of the stopwords provided by default.\n",
    "# #Con relative_scaling = 0, solo se consideran los rangos de las palabras. Si modificamos esto a relative_scaling = 1.0, entonces una palabra que aparece dos veces más frecuentemente aparecerá dos veces el tamaño. Por defecto, relative_scaling = 0.5.\n",
    "# wordcloud = WordCloud(relative_scaling=1.0, stopwords={'to', 'of'}).generate(text)\n",
    "wordcloud = WordCloud(relative_scaling=1.0,stopwords=stopwords).generate(text)\n",
    "\n",
    "#Finally, use matplotlib to render the word cloud:\n",
    "#plot_wordcloud(wordcloud)\n",
    "wordcloudfilename=input+\".png\"\n",
    "#wordcloudfilename=\"resume.png\"\n",
    "wordcloud.to_file(wordcloudfilename) \n",
    "\n",
    "###################analizador morfosintactico#################################################\n",
    "palabras_diferentes = []\n",
    "doc=stanford(text)\n",
    "for sent in doc.sentences:\n",
    "    #Por cada sentencia\n",
    "    for entry in sent.words:\n",
    "        definition = ''\n",
    "        synonyms = []\n",
    "        offset = 0\n",
    "        nueve = '_' \n",
    "        url = ''              \n",
    "        #Por cada palabra\n",
    "        if entry.text.isalpha() and (entry.upos == 'NOUN' or entry.upos == 'VERB') and entry.text.lower() not in palabras_diferentes:\n",
    "            #Si la palabra es un content word\n",
    "            palabras_diferentes.append(entry.text.lower())\n",
    "            wordfrequency = zipf_frequency(entry.text.lower(), 'en')\n",
    "            #lemafrequency = zipf_frequency(entry.lemma, 'en')\n",
    "            #Alto<=3 (1 por millon)\n",
    "            #Medio<=4 (10 por millon) \n",
    "            #Bajo<=5 (100 por millon)\n",
    "            if wordfrequency <= int(difficult):\n",
    "                lema=entry.lemma\n",
    "                synset_ids = wn.synsets(lema)\n",
    "                if entry.upos=='NOUN':\n",
    "                    patron='n'\n",
    "                if entry.upos=='VERB':\n",
    "                    patron='v'\n",
    "                #intentamos obtener la synset desambiguada, y la def,exam,sin and img-url sobre esa synset\n",
    "                try:\n",
    "                    synset_desambiguado = lesk(text.split(), entry.text, pos=patron, synsets=synset_ids)\n",
    "                    #definicion\n",
    "                    definition = synset_desambiguado.definition()\n",
    "                    #ejemplos\n",
    "                    examples = synset_desambiguado.examples()\n",
    "                    #offset\n",
    "                    offset = synset_desambiguado.offset()\n",
    "                    #traduccion es\n",
    "                    traduccion_es=answer2.lemmas('spa')\n",
    "                    es_list=[]\n",
    "                    for l in traduccion_es:\n",
    "                        es_list.append(l.name())\n",
    "                    #traduccion eu\n",
    "                    traduccion_eu=answer2.lemmas('eus') \n",
    "                    eu_list=[]\n",
    "                    for l in traduccion_eu:\n",
    "                        eu_list.append(l.name())\n",
    "                    #sinonimos\n",
    "                    for l in synset_desambiguado.lemma_names():\n",
    "                        synonyms.append(l.lower())\n",
    "                    try:\n",
    "                        synonyms.remove(entry.text.lower())\n",
    "                    except:\n",
    "                        pass\n",
    "                    try:\n",
    "                        synonyms.remove(entry.lemma)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    #url imagen\n",
    "                    if entry.upos=='NOUN':\n",
    "                        #deia wikidata\n",
    "                        url=wikidata_offset2url(offset)\n",
    "                        if url == '':\n",
    "                            url=wikipedia_offset2url(entry.text)\n",
    "                #si no desambiguamos\n",
    "                except:\n",
    "                    count = 0\n",
    "                    for synset in synset_ids:\n",
    "                        if entry.upos=='NOUN':\n",
    "                            patron='.n.'\n",
    "                        if entry.upos=='VERB':\n",
    "                            patron='.v.'\n",
    "\n",
    "                        if patron in synset.name() and count == 0:\n",
    "                            count = 1\n",
    "                            for l in synset.lemmas():\n",
    "                                synonyms.append(l.name())\n",
    "\n",
    "                            try:\n",
    "                                synonyms.remove(entry.text.lower())\n",
    "                            except:\n",
    "                                pass\n",
    "                            try:\n",
    "                                synonyms.remove(entry.lemma)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                            definition = synset.definition()\n",
    "                            offset = synset.offset()\n",
    "                            #ejemplos\n",
    "                            examples = synset.examples()\n",
    "                            #traduccion es\n",
    "                            traduccion_es=synset.lemmas('spa')\n",
    "                            es_list=[]\n",
    "                            for l in traduccion_es:\n",
    "                                es_list.append(l.name())\n",
    "                            #traduccion eu\n",
    "                            traduccion_eu=synset.lemmas('eus') \n",
    "                            eu_list=[]\n",
    "                            for l in traduccion_eu:\n",
    "                                eu_list.append(l.name())\n",
    "                            #url imagen\n",
    "                            if entry.upos=='NOUN':\n",
    "                                #deia wikidata\n",
    "                                url=wikidata_offset2url(offset)\n",
    "                                if url == '':\n",
    "                                    url=wikipedia_offset2url(entry.text)\n",
    "\n",
    "        if not set_is_empty(set(synonyms)):\n",
    "            nueve=entry.text+\":\"+str(set(synonyms))\n",
    "        else:\n",
    "            nueve=\"_\"\n",
    "        \n",
    "        print(str(entry.index)+\"\\t\"+entry.text+\"\\t\"+entry.lemma+\"\\t\"+entry.upos+\"\\t\"+entry.xpos+\"\\t\"+entry.feats+\"\\t\"+str(entry.governor)+\"\\t\"+str(entry.dependency_relation)+\"\\t\"+definition+\"\\t\"+nueve+\"\\t\"+str(url)+\"\\t\"+str(offset))\n",
    "        estfile.write(\"%s\" % str(entry.index)+\"\\t\"+entry.text+\"\\t\"+entry.lemma+\"\\t\"+entry.upos+\"\\t\"+entry.xpos+\"\\t\"+entry.feats+\"\\t\"+str(entry.governor)+\"\\t\"+str(entry.dependency_relation[:2])+\"\\t\"+definition+\"\\t\"+nueve+\"\\t\"+str(url)+\"\\t\"+str(offset))\n",
    "        estfile.write(\"\\n\")\n",
    "    estfile.write(\"\\n\")\n",
    "estfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertir svg a png\n",
    "import cairo\n",
    "import rsvg\n",
    "from xml.dom import minidom\n",
    "\n",
    "\n",
    "def convert_svg_to_png(svg_file, output_file):\n",
    "    # Get the svg files content\n",
    "    with open(svg_file) as f:\n",
    "        svg_data = f.read()\n",
    "\n",
    "    # Get the width / height inside of the SVG\n",
    "    doc = minidom.parse(svg_file)\n",
    "    width = int([path.getAttribute('width') for path\n",
    "                 in doc.getElementsByTagName('svg')][0])\n",
    "    height = int([path.getAttribute('height') for path\n",
    "                  in doc.getElementsByTagName('svg')][0])\n",
    "    doc.unlink()\n",
    "\n",
    "    # create the png\n",
    "    img = cairo.ImageSurface(cairo.FORMAT_ARGB32, width, height)\n",
    "    ctx = cairo.Context(img)\n",
    "    handler = rsvg.Handle(None, str(svg_data))\n",
    "    handler.render_cairo(ctx)\n",
    "    img.write_to_png(output_file)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from argparse import ArgumentParser\n",
    "\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"-f\", \"--file\", dest=\"svg_file\",\n",
    "                        help=\"SVG input file\", metavar=\"FILE\")\n",
    "    parser.add_argument(\"-o\", \"--output\", dest=\"output\", default=\"svg.png\",\n",
    "                        help=\"PNG output file\", metavar=\"FILE\")\n",
    "    args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://upload.wikimedia.org/wikipedia/commons/d/df/2018_06_TropicalIslands_IMG_2170.jpg\n",
      "name:\n",
      "ext:\n",
      "http://en.wikipedia.org/w/api.php?action=query&titles=dog&generator=images&gimlimit=10&prop=imageinfo&iiprop=url|dimensions|mime&format=json\n"
     ]
    }
   ],
   "source": [
    "#wikipedia: pip install wikipedia\n",
    "#url imagen\n",
    "##################IMAGES##################################################################\n",
    "import wikipedia\n",
    "\n",
    "#set_lang() method is used to set the language that you want to set.It takes an argument that is prefix of the language like for arabic prefix is ar and so on.\n",
    "#If you want to know about the prefix of the different languages then refer this link but make sure that the Wikipedia should have that article in the language you want.\n",
    "#Here i have set the language arabic and article is facebook.It will give the summary of facebook in arabic language that’s so amazing.\n",
    "#wikipedia.set_lang(\"eus\")\n",
    "#wikipedia.summary(\"wolf\")\n",
    "\n",
    "upos='NOUN'\n",
    "text='dog'\n",
    "ext =''\n",
    "svg = False\n",
    "name=''\n",
    "i = 0\n",
    "if upos=='NOUN':\n",
    "     while (svg==False):\n",
    "        try:\n",
    "            #First of all wikipedia.page() will store all the relevant informations \n",
    "            #from the requested page in the variable imagepage. \n",
    "            imagepage=wikipedia.page(text)\n",
    "            #Existen muchas funciones. imagepage.url devuelve la url, imagepage.url.content el contenido, title el título....\n",
    "            #imagepage.images[0] will return the URL of the image that is present at index 0. \n",
    "            #If you want to fetch another image use index as 1, 2, 3, etc, according to images present in the page.\n",
    "            url = imagepage.images[i]\n",
    "            name, ext =(url.split('/')[-1].split('.'))\n",
    "            if ext=='.svg':\n",
    "                 i = i+1\n",
    "            else:\n",
    "                 svg = True\n",
    "        except:\n",
    "            svg = True\n",
    "print(str(url))\n",
    "print(\"name:\"+name)\n",
    "print(\"ext:\"+ext)\n",
    "#segunda opcion:\n",
    "url2=\"http://en.wikipedia.org/w/api.php?action=query&titles=\"+text+\"&generator=images&gimlimit=10&prop=imageinfo&iiprop=url|dimensions|mime&format=json\" \n",
    "#Nos podemos bajar el fichero json y obtener la imagen que mejor nos convenga.\n",
    "#Solo hay que cambiar la parte de titles\n",
    "print(str(url2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\item Wikidata\n",
    "El repositorio Wikidata consta principalmente de elementos, cada uno conteniendo una etiqueta, una descripción y cualquier número de alias. Los elementos son identificados de manera única por un prefijo Q seguido por un número, como Douglas Adams (Q42). Las declaraciones describen características detalladas de cada elemento y constan de una propiedad y un valor. Las propiedades en Wikidata tienen un prefijo P seguido de un número, como con educated at (P69). \n",
    "Por ejemplo:\n",
    "Elementos \tPropiedad \tValor\n",
    "Q42 \tP69 \tQ691283\n",
    "Douglas Adams \teducated at \tSt John's College \n",
    "\n",
    "Mediante la propiedad P2888 se puede unir los elementos de Wikidata con WordNet (mediante la \"exact match(P2888)\" usado para conectar dos conceptos, indica un alto grado de confianza de que el concepto puede ser usado de forma intercambiable)\n",
    "\t\n",
    "Elementos \tPropiedad \tValor\n",
    "Q322787 \tP69 \tQ691283\n",
    "mashed potato (exact match)P2888 http://wordnet-rdf.princeton.edu/wn30/07711569-n\n",
    "\n",
    "También puedo acceder a las imagenes desde wikidata:\n",
    "%image of relevant illustration of the subject; if available, use more specific properties (sample: coat of arms image, locator map, flag image, signature image, logo image, collage image); only images which exist on Wikimedia Commons are acceptable\n",
    "Elementos \tPropiedad \tValor\n",
    "Q322787 \t image (P18)  Aligot.jpg\n",
    "También se puede acceder a BabelNet (\\url{http://babelnet.org}\n",
    "Elementos \tPropiedad \tValor\n",
    "https://www.wikidata.org/wiki/Q3490685 \t  BabelNet ID (P2581)   https://babelnet.org/synset?word=bn:00039711n\n",
    "\n",
    "¿Podriamos acceder directamente desde WordNet a BabelNet?\n",
    "No, ya que no utiliza los mismos identificadores.\n",
    "\n",
    "¿Puedo acceder desde wordnet a wikidata images?\n",
    "Si usando Wikidata Query Service (WDQS) SPARQL query:\n",
    "https://query.wikidata.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q322787\n",
      "potato dish\n",
      "https://upload.wikimedia.org/wikipedia/commons/7/71/Aligot.jpg\n"
     ]
    }
   ],
   "source": [
    "#Wikidata\n",
    "#Wikidata query service -> code in python \n",
    "#https://pypi.org/project/Wikidata/\n",
    "# pip install sparqlwrapper\n",
    "# https://rdflib.github.io/sparqlwrapper/\n",
    "\n",
    "import wikidata\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from wikidata.client import Client\n",
    "\n",
    "\n",
    "#url=\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?query={\"+consulta\n",
    "\n",
    "#query = \"\"\"Select * where {?item wdt:P2888 <http://wordnet-rdf.princeton.edu/wn30/07711569-n> } \"\"\"\n",
    "def get_results(endpoint_url, query):\n",
    "    sparql = SPARQLWrapper(endpoint_url)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "\n",
    "#offset=7711569\n",
    "endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "soffset=str(offset).zfill(8)\n",
    "query=\"Select * where {?item wdt:P2888 <http://wordnet-rdf.princeton.edu/wn30/\"+soffset+\"-n> }\"\n",
    "results = get_results(endpoint_url, query)\n",
    "ema=results[\"results\"][\"bindings\"]\n",
    "item=ema[0].get('item')\n",
    "value=item.get('value')\n",
    "head, tail = value.split(\"http://www.wikidata.org/entity/\")\n",
    "print(tail)\n",
    "#for result in results[\"results\"][\"bindings\"]:\n",
    "#    print(result)\n",
    "client = Client()\n",
    "entity = client.get(tail, load=True)\n",
    "print(entity.description)\n",
    "image_prop = client.get('P18')\n",
    "image = entity[image_prop]\n",
    "#image-><wikidata.commonsmedia.File 'File:KBS \"The Producers\" press conference, 11 May 2015 10.jpg'>\n",
    "image.image_resolution\n",
    "print(str(image.image_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-3dca22d3506b>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-3dca22d3506b>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    import urllibpage = requests.get(\"http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n07753592\")\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#ImageNet\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "def imagenet_offset2url(offset):\n",
    "    soffset=str(offset).zfill(8)\n",
    "    query=\"http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n\"+str(soffset)\n",
    "    page = requests.get(query)#ship synset\n",
    "    # BeautifulSoup is an HTML parsing library\n",
    "    #puts the content of the website into the soup variable, each url on a different line\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    str_soup=str(soup)#convert soup to string so it can be split\n",
    "    split_urls=str_soup.split('\\r\\n')#split so each url is a different possition on a list\n",
    "    return(split_urls[0])\n",
    "imagenet_offset2url(4194289) #ship synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/kepa/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/kepa/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/kepa/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/kepa/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/kepa/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/home/kepa/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      " Fry the garlic and the chilly in the oil on a moderate heat .\n",
      "['Fry', 'the', 'garlic', 'and', 'the', 'chilly', 'in', 'the', 'oil', 'on', 'a', 'moderate', 'heat', '.']\n",
      "Definición:\n",
      "cook on a hot surface using fat\n",
      "Ejemplos:\n",
      "['fry the pancakes']\n",
      "Traduccion:\n",
      "['freír']\n",
      "['frijitu']\n",
      "Fry\tfry\tVERB\tVB\tMood=Imp|VerbForm=Fin\t0\troot\tcook on a hot surface using fat\t_\thttp://image-net.org/synset?wnid=v00325328\n",
      "the\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t3\tdet\t\t_\t\n",
      "Definición:\n",
      "aromatic bulb used as seasoning\n",
      "Ejemplos:\n",
      "[]\n",
      "Traduccion:\n",
      "[]\n",
      "['baratxuri', 'berakatz']\n",
      "garlic\tgarlic\tNOUN\tNN\tNumber=Sing\t1\tobj\taromatic bulb used as seasoning\tgarlic:{'ail'}\thttp://image-net.org/synset?wnid=n07818277\n",
      "and\tand\tCCONJ\tCC\t_\t6\tcc\t\t_\t\n",
      "the\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t6\tdet\t\t_\t\n",
      "chilly\tchilly\tADJ\tJJ\tDegree=Pos\t3\tconj\t\t_\t\n",
      "in\tin\tADP\tIN\t_\t9\tcase\t\t_\t\n",
      "the\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t9\tdet\t\t_\t\n",
      "Definición:\n",
      "a slippery or viscous liquid or liquefiable substance not miscible with water\n",
      "Ejemplos:\n",
      "[]\n",
      "Traduccion:\n",
      "['aceite']\n",
      "['olio']\n",
      "oil\toil\tNOUN\tNN\tNumber=Sing\t6\tobl\ta slippery or viscous liquid or liquefiable substance not miscible with water\t_\thttp://image-net.org/synset?wnid=n14966667\n",
      "on\ton\tADP\tIN\t_\t13\tcase\t\t_\t\n",
      "a\ta\tDET\tDT\tDefinite=Ind|PronType=Art\t13\tdet\t\t_\t\n",
      "moderate\tmoderate\tADJ\tJJ\tDegree=Pos\t13\tamod\t\t_\t\n",
      "Definición:\n",
      "a preliminary race in which the winner advances to a more important race\n",
      "Ejemplos:\n",
      "[]\n",
      "Traduccion:\n",
      "[]\n",
      "[]\n",
      "heat\theat\tNOUN\tNN\tNumber=Sing\t1\tobl\ta preliminary race in which the winner advances to a more important race\t_\thttp://image-net.org/synset?wnid=n07461288\n",
      ".\t.\tPUNCT\t.\t_\t1\tpunct\t\t_\t\n",
      " Add the tomato sauce .\n",
      "['Add', 'the', 'tomato', 'sauce', '.']\n",
      "Definición:\n",
      "bestow a quality on\n",
      "Ejemplos:\n",
      "['Her presence lends a certain cachet to the company', 'The music added a lot to the play', 'She brings a special atmosphere to our meetings', 'This adds a light note to the program']\n",
      "Traduccion:\n",
      "['aportar', 'contribuir', 'propiciar']\n",
      "['ekarpena_egin', 'eman']\n",
      "Add\tadd\tVERB\tVB\tMood=Imp|VerbForm=Fin\t0\troot\tbestow a quality on\tAdd:{'impart', 'bring', 'lend', 'bestow', 'contribute'}\thttp://image-net.org/synset?wnid=v02324478\n",
      "the\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t4\tdet\t\t_\t\n",
      "Definición:\n",
      "native to South America; widely cultivated in many varieties\n",
      "Ejemplos:\n",
      "[]\n",
      "Traduccion:\n",
      "['solanum_lycopersicum']\n",
      "['tomate']\n",
      "tomato\ttomato\tNOUN\tNN\tNumber=Sing\t4\tcompound\tnative to South America; widely cultivated in many varieties\ttomato:{'tomato_plant', 'love_apple', 'lycopersicon_esculentum'}\thttp://image-net.org/synset?wnid=n12905817\n",
      "Definición:\n",
      "flavorful relish or dressing or topping served as an accompaniment to food\n",
      "Ejemplos:\n",
      "[]\n",
      "Traduccion:\n",
      "['aderezo', 'salsa']\n",
      "['ongailu', 'saltsa']\n",
      "sauce\tsauce\tNOUN\tNN\tNumber=Sing\t1\tobj\tflavorful relish or dressing or topping served as an accompaniment to food\t_\thttp://image-net.org/synset?wnid=n07829412\n",
      ".\t.\tPUNCT\t.\t_\t1\tpunct\t\t_\t\n",
      " Cook the sauce for about 15 minutes .\n",
      "['Cook', 'the', 'sauce', 'for', 'about', '15', 'minutes', '.']\n",
      "Definición:\n",
      "tamper, with the purpose of deception\n",
      "Ejemplos:\n",
      "['Fudge the figures', 'cook the books', 'falsify the data']\n",
      "Traduccion:\n",
      "['adulterar', 'desfigurar', 'falsear', 'falsificar', 'fingir', 'hacer_trampa', 'manipular']\n",
      "[]\n",
      "Cook\tCook\tVERB\tVB\tMood=Imp|VerbForm=Fin\t0\troot\ttamper, with the purpose of deception\tCook:{'fake', 'misrepresent', 'fudge', 'falsify', 'wangle', 'manipulate'}\thttp://image-net.org/synset?wnid=v02576921\n",
      "the\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t3\tdet\t\t_\t\n",
      "sauce\tsauce\tNOUN\tNN\tNumber=Sing\t1\tobj\t\t_\t\n",
      "for\tfor\tADP\tIN\t_\t7\tcase\t\t_\t\n",
      "about\tabout\tADV\tRB\t_\t6\tadvmod\t\t_\t\n",
      "15\t15\tNUM\tCD\tNumType=Card\t7\tnummod\t\t_\t\n",
      "Definición:\n",
      "a particular point in time\n",
      "Ejemplos:\n",
      "['the moment he arrived the party began']\n",
      "Traduccion:\n",
      "['instante', 'momento']\n",
      "[]\n",
      "minutes\tminute\tNOUN\tNNS\tNumber=Plur\t1\tobl\ta particular point in time\tminutes:{'second', 'instant', 'moment'}\thttp://image-net.org/synset?wnid=n15244650\n",
      ".\t.\tPUNCT\t.\t_\t1\tpunct\t\t_\t\n",
      " Cook the pasta in boiling water .\n",
      "['Cook', 'the', 'pasta', 'in', 'boiling', 'water', '.']\n",
      "Cook\tCook\tVERB\tVB\tMood=Imp|VerbForm=Fin\t0\troot\t\t_\t\n",
      "the\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t3\tdet\t\t_\t\n",
      "Definición:\n",
      "shaped and dried dough made from flour and water and sometimes egg\n",
      "Ejemplos:\n",
      "[]\n",
      "Traduccion:\n",
      "['pasta']\n",
      "['pasta']\n",
      "pasta\tpasta\tNOUN\tNN\tNumber=Sing\t1\tobj\tshaped and dried dough made from flour and water and sometimes egg\tpasta:{'alimentary_paste'}\thttp://image-net.org/synset?wnid=n07698915\n",
      "in\tin\tSCONJ\tIN\t_\t5\tmark\t\t_\t\n",
      "Definición:\n",
      "immerse or be immersed in a boiling liquid, often for cooking purposes\n",
      "Ejemplos:\n",
      "['boil potatoes', 'boil wool']\n",
      "Traduccion:\n",
      "['cocer']\n",
      "['egosi']\n",
      "boiling\tboiling\tVERB\tVBG\tVerbForm=Ger\t1\tadvcl\timmerse or be immersed in a boiling liquid, often for cooking purposes\tboiling:{'boil'}\thttp://image-net.org/synset?wnid=v00328128\n",
      "Definición:\n",
      "a liquid necessary for the life of most animals and plants\n",
      "Ejemplos:\n",
      "['he asked for a drink of water']\n",
      "Traduccion:\n",
      "['agua', 'H2O']\n",
      "['H2O', 'ur']\n",
      "water\twater\tNOUN\tNN\tNumber=Sing\t5\tobj\ta liquid necessary for the life of most animals and plants\t_\thttp://image-net.org/synset?wnid=n07935504\n",
      ".\t.\tPUNCT\t.\t_\t1\tpunct\t\t_\t\n",
      " Strain the pasta through and mix well with the sauce .\n",
      "['Strain', 'the', 'pasta', 'through', 'and', 'mix', 'well', 'with', 'the', 'sauce', '.']\n",
      "Definición:\n",
      "rub through a strainer or process in an electric blender\n",
      "Ejemplos:\n",
      "['puree the vegetables for the baby']\n",
      "Traduccion:\n",
      "[]\n",
      "[]\n",
      "Strain\tStrain\tVERB\tVB\tMood=Imp|VerbForm=Fin\t0\troot\trub through a strainer or process in an electric blender\tStrain:{'puree'}\thttp://image-net.org/synset?wnid=v01250474\n",
      "the\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t3\tdet\t\t_\t\n",
      "pasta\tpasta\tNOUN\tNN\tNumber=Sing\t1\tobj\t\t_\t\n",
      "through\tthrough\tADV\tRB\t_\t1\tcompound:prt\t\t_\t\n",
      "and\tand\tCCONJ\tCC\t_\t6\tcc\t\t_\t\n",
      "Definición:\n",
      "mix so as to make a random order or arrangement\n",
      "Ejemplos:\n",
      "['shuffle the cards']\n",
      "Traduccion:\n",
      "['mezclar', 'revolver']\n",
      "['astindu', 'nahasi', 'nahastu']\n",
      "mix\tmix\tVERB\tVB\tMood=Imp|VerbForm=Fin\t1\tconj\tmix so as to make a random order or arrangement\tmix:{'shuffle', 'ruffle'}\thttp://image-net.org/synset?wnid=v01418667\n",
      "well\twell\tADV\tRB\tDegree=Pos\t6\tadvmod\t\t_\t\n",
      "with\twith\tADP\tIN\t_\t10\tcase\t\t_\t\n",
      "the\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t10\tdet\t\t_\t\n",
      "sauce\tsauce\tNOUN\tNN\tNumber=Sing\t6\tobl\t\t_\t\n",
      ".\t.\tPUNCT\t.\t_\t1\tpunct\t\t_\t\n",
      " Serve the pasta in bowls with the parsley and the cheese .\n",
      "['Serve', 'the', 'pasta', 'in', 'bowls', 'with', 'the', 'parsley', 'and', 'the', 'cheese', '.']\n",
      "Definición:\n",
      "work for or be a servant to\n",
      "Ejemplos:\n",
      "['May I serve you?', 'She attends the old lady in the wheelchair', 'Can you wait on our table, please?', 'Is a salesperson assisting you?', 'The minister served the King for many years']\n",
      "Traduccion:\n",
      "['asistir', 'atender', 'cuidar', 'servir']\n",
      "[]\n",
      "Serve\tserve\tVERB\tVB\tMood=Imp|VerbForm=Fin\t0\troot\twork for or be a servant to\tServe:{'wait_on', 'attend_to', 'attend', 'assist'}\thttp://image-net.org/synset?wnid=v02540670\n",
      "the\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t3\tdet\t\t_\t\n",
      "pasta\tpasta\tNOUN\tNN\tNumber=Sing\t1\tobj\t\t_\t\n",
      "in\tin\tADP\tIN\t_\t5\tcase\t\t_\t\n",
      "Definición:\n",
      "a large ball with finger holes used in the sport of bowling\n",
      "Ejemplos:\n",
      "[]\n",
      "Traduccion:\n",
      "[]\n",
      "['bola']\n",
      "bowls\tbowl\tNOUN\tNNS\tNumber=Plur\t1\tobl\ta large ball with finger holes used in the sport of bowling\tbowls:{'bowling_ball'}\thttp://image-net.org/synset?wnid=n02882301\n",
      "with\twith\tADP\tIN\t_\t8\tcase\t\t_\t\n",
      "the\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t8\tdet\t\t_\t\n",
      "Definición:\n",
      "aromatic herb with flat or crinkly leaves that are cut finely and used to garnish food\n",
      "Ejemplos:\n",
      "[]\n",
      "Traduccion:\n",
      "[]\n",
      "['perrexil']\n",
      "parsley\tparsley\tNOUN\tNN\tNumber=Sing\t1\tobl\taromatic herb with flat or crinkly leaves that are cut finely and used to garnish food\t_\thttp://image-net.org/synset?wnid=n07819896\n",
      "and\tand\tCCONJ\tCC\t_\t11\tcc\t\t_\t\n",
      "the\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t11\tdet\t\t_\t\n",
      "Definición:\n",
      "erect or decumbent Old World perennial with axillary clusters of rosy-purple flowers; introduced in United States\n",
      "Ejemplos:\n",
      "[]\n",
      "Traduccion:\n",
      "[]\n",
      "['malba']\n",
      "cheese\tcheese\tNOUN\tNN\tNumber=Sing\t8\tconj\terect or decumbent Old World perennial with axillary clusters of rosy-purple flowers; introduced in United States\tcheese:{'high_mallow', 'tall_mallow', 'cheeseflower', 'malva_sylvestris'}\thttp://image-net.org/synset?wnid=n12171503\n",
      ".\t.\tPUNCT\t.\t_\t1\tpunct\t\t_\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/kepa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /home/kepa/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "####input file##################\n",
    "input=\"receta.txt\" #sys.argv[1]\n",
    "####ouput files#################\n",
    "#estadisticos\n",
    "estadisticaoutput=input+\".out.csv\"\n",
    "\n",
    "#Write all the information in the file\n",
    "estfile = open(estadisticaoutput, \"w\")\n",
    "\n",
    "\n",
    "###############Tratamiento de texto###############################################\n",
    "#quitar todos los retornos \\n si contiene\n",
    "text = open(input).read().replace('\\n', '')\n",
    "#remove text inside parentheses\n",
    "#text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "#separa , . ! ( ) ? ; del texto con espacios, teniendo en cuenta que los no son numeros en el caso de , y . \n",
    "text = re.sub(r'[.]+(?![0-9])', r' . ', text)\n",
    "text = re.sub(r'[,]+(?![0-9])', r' , ', text)\n",
    "text = re.sub(r\"!\", \" ! \", text)\n",
    "text = re.sub(r\"\\(\", \" ( \", text)\n",
    "text = re.sub(r\"\\)\", \" ) \", text)\n",
    "text = re.sub(r\"\\?\", \" ? \", text)\n",
    "text = re.sub(r\";\", \" ; \", text)\n",
    "#sustituye 2 espacios seguidos por 1\n",
    "text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "################STOPWORDS##############################################################################\n",
    "#Generating a word cloud with no optional parameters based on the above string:\n",
    "from wordcloud import STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "def set_is_empty(some_set):\n",
    "    return some_set == set()\n",
    "#stopwords.add(\"every\")\n",
    "#stopwords.add(\"will\")\n",
    "#stopwords={'to', 'of', 'us'}\n",
    "##############WORDCLOUD########################################################################\n",
    "#matplotlib kendu dezakegu \n",
    "import matplotlib\n",
    "from wordcloud import WordCloud\n",
    "#Generating a word cloud with no optional parameters based on the above string:\n",
    "# #This is because the wordcloud module ignores stopwords by default. Refer to Part 1 of the NLTK tutorial if the concept of stopwords is new to you.If we wish, we can specify our own set of stopwords, instead of the stopwords provided by default.\n",
    "# #Con relative_scaling = 0, solo se consideran los rangos de las palabras. Si modificamos esto a relative_scaling = 1.0, entonces una palabra que aparece dos veces más frecuentemente aparecerá dos veces el tamaño. Por defecto, relative_scaling = 0.5.\n",
    "# wordcloud = WordCloud(relative_scaling=1.0, stopwords={'to', 'of'}).generate(text)\n",
    "wordcloud = WordCloud(relative_scaling=1.0,stopwords=stopwords).generate(text)\n",
    "#Finally, use matplotlib to render the word cloud:\n",
    "#plot_wordcloud(wordcloud)\n",
    "wordcloudfilename=input+\".png\"\n",
    "#wordcloudfilename=\"resume.png\"\n",
    "wordcloud.to_file(wordcloudfilename) \n",
    "###################WORDFREQ#################################################################\n",
    "from wordfreq import zipf_frequency\n",
    "\n",
    "##################PARSER#############################################################\n",
    "import stanfordnlp\n",
    "#stanfordnlp.download(\"en\")\n",
    "stanford=stanfordnlp.Pipeline()\n",
    "\n",
    "##################WORDNET############################################################ \n",
    "#Instalar 'wordnet' \n",
    "nltk.download('wordnet')\n",
    "#Add multilingual wordnet\n",
    "nltk.download('omw')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def set_is_empty(some_set):\n",
    "    return some_set == set()\n",
    "###################WSD########################################################################\n",
    "#lesk erabiltzeko\n",
    "from nltk.wsd import lesk\n",
    "#simple_lesk erabiltzeko\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#from pywsd.lesk import simple_lesk\n",
    "\n",
    "##################IMAGES##################################################################\n",
    "import wikipedia\n",
    "###################MAIN#################################################\n",
    "palabras_raras = []\n",
    "palabras_diferentes = []\n",
    "doc=stanford(text)\n",
    "for sent in doc.sentences:\n",
    "\t#Por cada sentencia\n",
    "\tsentence=\"\"\n",
    "\tfor entry in sent.words:\n",
    "\t\tsentence=sentence+' '+entry.text\n",
    "\tprint(sentence)\n",
    "\tprint(sentence.split())\n",
    "\tfor entry in sent.words:\n",
    "\t\tnueve='_'\n",
    "\t\tdefinicion= ''\n",
    "\t\tsynonyms = []\n",
    "\t\toffset=0\n",
    "\t\turl=''\n",
    "\t\t#Por cada palabra\n",
    "\t\tif entry.text.isalpha() and (entry.upos == 'NOUN' or entry.upos == 'VERB') and entry.text.lower() not in palabras_diferentes:\n",
    "\t\t\t#Si la palabra es un content word\n",
    "\t\t\tpalabras_diferentes.append(entry.text.lower())\n",
    "\t\t\twordfrequency = zipf_frequency(entry.text.lower(), 'en')\n",
    "\t\t\t#lemafrequency = zipf_frequency(entry.lemma, 'en')\n",
    "\t\t\tif wordfrequency <= 8:\n",
    "\t\t\t\t#Si es rara\n",
    "\t\t\t\tpalabras_raras.append(entry.text)\n",
    "\t\t\t\t#Todos los synsets del lema \n",
    "\t\t\t\tsynset_ids=wn.synsets(entry.lemma)\n",
    "\t\t\t\tif entry.upos=='NOUN':\n",
    "\t\t\t\t\tpatron='n'\n",
    "\t\t\t\t\t#synset_ids=wn.synsets(entry.lemma, pos=wn.NOUN)\n",
    "\t\t\t\tif entry.upos=='VERB':\n",
    "\t\t\t\t\tpatron='v'\n",
    "\t\t\t\t\t#synset_ids=wn.synsets(entry.lemma, pos=wn.VERB)\n",
    "\t\t\t\t#print(entry.lemma+\":\"+str(synset_ids))\n",
    "\t\t\t\t#Desambiguador\n",
    "\t\t\t\t#print('\\n',str(entry.upos))                    \n",
    "\t\t\t\t#answer1=lesk(sentence,entry.text)\n",
    "\t\t\t\t#print('\\n',str(answer1))\n",
    "\t\t\t\t#print('\\t',answer1.definition())\n",
    "\t\t\t\t#answer2 = simple_lesk(sentence, entry.text, pos=patron)\n",
    "\t\t\t\t#print('\\n',str(answer2))\n",
    "\t\t\t\t#print('\\t',answer2.definition())\n",
    "\t\t\t\t#definicion=answer2.definition()\n",
    "\t\t\t\t#print('\\t',answer2.lemma_names())\n",
    "\t\t\t\t#print('\\t',answer2.offset())\n",
    "\t\t\t\t#INTENTO DESAMBIGUAR CON LESK            \n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tanswer2 = lesk(text.split(), entry.text, pos=patron, synsets=synset_ids)\n",
    "\t\t\t\t\tdefinicion=answer2.definition()\n",
    "\t\t\t\t\tprint(\"Definición:\")\n",
    "\t\t\t\t\tprint(definicion)\n",
    "\t\t\t\t\tejemplos=answer2.examples()\n",
    "\t\t\t\t\tprint(\"Ejemplos:\")\n",
    "\t\t\t\t\tprint(ejemplos)\n",
    "\t\t\t\t\tprint(\"Traduccion:\")                    \n",
    "\t\t\t\t\ttraduccion_es=answer2.lemmas('spa') #.split('.')[3]\n",
    "\t\t\t\t\tes_list=[]\n",
    "\t\t\t\t\tfor l in traduccion_es:\n",
    "\t\t\t\t\t\tes_list.append(l.name())\n",
    "\t\t\t\t\tprint(es_list)                    \n",
    "\t\t\t\t\ttraduccion_eu=answer2.lemmas('eus') #.split('.')[3]\n",
    "\t\t\t\t\teu_list=[]\n",
    "\t\t\t\t\tfor l in traduccion_eu:\n",
    "\t\t\t\t\t\teu_list.append(l.name())\n",
    "\t\t\t\t\tprint(eu_list) \n",
    "#\t\t\t\t\tprint(\"Lemmas_names:\")                    \n",
    "#\t\t\t\t\tprint(answer2.lemmas_names()) \n",
    "#\\item De ImageNet \\cite{imagenet} que es una colección de imágenes etiquetadas contra WordNet 3.0 y descritas en http://image-net.org. Contiene 14197122 imágenes y 21841 synsets indexados.Por ejemplo:platano\n",
    "#\\url{http://image-net.org/explore.php?wnid=n07753592}\n",
    "#\\url{http://wordnet-rdf.princeton.edu/wn30/07753592-n}\n",
    "\n",
    "\t\t\t\t\tif entry.upos=='NOUN':\n",
    "\t\t\t\t\t\turl=\"http://image-net.org/explore.php?wnid=n\"+str(answer2.offset()).zfill(8)\n",
    "\t\t\t\t\t\turl=\"http://image-net.org/synset?wnid=n\"+str(answer2.offset()).zfill(8)\n",
    "\t\t\t\t\tif entry.upos=='VERB':\n",
    "\t\t\t\t\t\turl=\"http://image-net.org/explore.php?wnid=v\"+str(answer2.offset()).zfill(8)\n",
    "\t\t\t\t\t\turl=\"http://image-net.org/synset?wnid=v\"+str(answer2.offset()).zfill(8) \n",
    "\t\t\t\t\t#offset=\"http://www.image-net.org/synset?wnid=\"+str(answer2.offset())\n",
    "\t\t\t\t\t#offset=\"015201505\" #str(answer2.offset())                          \n",
    "\t\t\t\t\t#sparql=\"SELECT * WHERE {?item wdt:P2888 <http://wordnet-rdf.princeton.edu/wn30/\"+offset+\"-n>}\"\n",
    "\t\t\t\t\t#url=\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?query=\"+sparql\n",
    "\t\t\t\t\t#try:\n",
    "\t\t\t\t\t\t#name=re.sub(r\"\\_\", \" \", answer2.name().split(\".\")[0])\n",
    "\t\t\t\t\t#\toffset=wikipedia.page(entry.text).images[0]\n",
    "\t\t\t\t\t#except:\n",
    "\t\t\t\t\t#\tpass                        \n",
    "\t\t\t\t\t#si tiene sinonimos\n",
    "\t\t\t\t\tfor l in answer2.lemma_names():\n",
    "\t\t\t\t\t\tsynonyms.append(l.lower())\n",
    "\t\t\t\t\t\t#print(l.lower())\n",
    "\t\t\t\t\t#print(entry.text.lower())\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\tsynonyms.remove(entry.text.lower())\n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\tpass\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\tsynonyms.remove(entry.lemma)\n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\tpass\n",
    "\t\t\t\t#COJO EL PRIMER SYNSET QUE CUMPLA LA RESTRICCIÓN POS\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tcount=0\n",
    "\t\t\t\t\tfor synset in synset_ids: # [Synset('libra.n.03'), Synset('libra.n.02'), Synset('pound.n.01')]\n",
    "\t\t\t\t\t\tif entry.upos=='NOUN':\n",
    "\t\t\t\t\t\t\tpatron='.n.'\n",
    "\t\t\t\t\t\tif entry.upos=='VERB':\n",
    "\t\t\t\t\t\t\tpatron='.v.'\n",
    "\t\t\t\t\t\tif patron in synset.name() and count==0:\n",
    "\t\t\t\t\t\t\tcount=1\n",
    "\t\t\t\t\t\t\t#print (\"synset.name():\")                 \n",
    "\t\t\t\t\t\t\t#print (synset.name())   #pound.n.01\n",
    "\t\t\t\t\t\t\t#sinonimoak bilatu\n",
    "\t\t\t\t\t\t\t#print (str(synset.lemmas(lang='eus')))\n",
    "\t\t\t\t\t\t\tfor l in synset.lemmas(): #[Lemma('pound.n.01.libera'), Lemma('pound.n.01.libra')]\n",
    "\t\t\t\t\t\t\t\tsynonyms.append(l.name('spa'))\n",
    "\t\t\t\t\t\t\t\t#print (\"l.name():\") #libera, libra              \n",
    "\t\t\t\t\t\t\t\t#print (l.name())\n",
    "\t\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\tsynonyms.remove(entry.text.lower())\n",
    "\t\t\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\t\t\tpass\n",
    "\t\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\tsynonyms.remove(entry.lemma)\n",
    "\t\t\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\t\t\tpass\n",
    "\t\t\t\t\t\t\tdefinicion=synset.definition()\n",
    "\t\t\t\t\t\t\tif entry.upos=='NOUN':\n",
    "\t\t\t\t\t\t\t\turl=\"http://image-net.org/explore.php?wnid=n\"+str(synset.offset()).zfill(8)\n",
    "\t\t\t\t\t\t\t\turl=\"http://image-net.org/synset?wnid=n\"+str(synset.offset()).zfill(8)\n",
    "\t\t\t\t\t\t\t\turl=\"http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=\" + str(synset.offset()).zfill(8)\n",
    "\t\t\t\t\t\t\tif entry.upos=='VERB':\n",
    "\t\t\t\t\t\t\t\turl=\"http://image-net.org/explore.php?wnid=v\"+str(synset.offset()).zfill(8)\n",
    "\t\t\t\t\t\t\t\turl=\"http://image-net.org/synset?wnid=v\"+str(synset.offset()).zfill(8)\n",
    "                            #str(1).zfill(3);\n",
    "\t\t\t\t\t\t\t#offset=str(synset.offset())\n",
    "\t\t\t\t\t\t\t#offset=\"015201505\" #str(answer2.offset())                          \n",
    "\t\t\t\t\t\t\t#sparql=\"SELECT * WHERE {?item wdt:P2888 <http://wordnet-rdf.princeton.edu/wn30/\"+offset+\"-n>}\"\n",
    "\t\t\t\t\t\t\t#url=\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?query=\"+sparql\n",
    "\t\t\t\t\t\t\t#offset=\"http://www.image-net.org/synset?wnid=\"+str(synset.offset())\n",
    "\t\t\t\t\t\t\t#try:\n",
    "\t\t\t\t\t\t\t\t#name=re.sub(r\"\\_\", \" \", synset.name().split(\".\")[0])\n",
    "\t\t\t\t\t\t\t\t#offset=wikipedia.page(name).images[0]\n",
    "\t\t\t\t\t\t\t#except:\n",
    "\t\t\t\t\t\t\t\t#pass\n",
    "\t\tif not set_is_empty(set(synonyms)):\n",
    "\t\t\tnueve=entry.text+\":\"+str(set(synonyms))\n",
    "\t\telse:\n",
    "\t\t\tnueve=\"_\"\n",
    "\t\tprint(entry.text+\"\\t\"+entry.lemma+\"\\t\"+entry.upos+\"\\t\"+entry.xpos+\"\\t\"+entry.feats+\"\\t\"+str(entry.governor)+\"\\t\"+str(entry.dependency_relation)+\"\\t\"+definicion+\"\\t\"+nueve+\"\\t\"+str(url))\n",
    "\t\testfile.write(\"%s\" % entry.text+\"\\t\"+entry.lemma+\"\\t\"+entry.upos+\"\\t\"+entry.xpos+\"\\t\"+entry.feats+\"\\t\"+str(entry.governor)+\"\\t\"+str(entry.dependency_relation[:2])+\"\\t\"+definicion+\"\\t\"+nueve+\"\\t\"+str(url))\n",
    "\t\testfile.write(\"\\n\")\n",
    "estfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Basque:\n",
    "Understanding WordNet: Instalar 'wordnet' baja muchos ficheros a /home/kepa/nltk_data/corpora/wordnet nltk.download('wordnet'). To add multilingual wordnet baja un fichero para el euskara a /home/kepa/nltk_data/corpora/omw/eus/wn-data-eus.tab nltk.download('omw') euskarazko lema=\"Libra\" /home/kepa/nltk_data/corpora/omw/eus/wn-data-eus.tab fitxategian:\n",
    "# Multilingual Central Repository       eus     http://adimen.si.ehu.es/web/MCR/        CC BY 3.0\n",
    "00001740-n      lemma   entitate\n",
    "00001740-n      lemma   izaki\n",
    "00001740-n      lemma   sorkari\n",
    "00001740-v      lemma   arnasa hartu\n",
    "00001740-v      lemma   arnastu\n",
    "00001740-v      lemma   hats hartu\n",
    "00002137-n      lemma   abstrakzio\n",
    "00002684-n      lemma   gauza\n",
    "00002684-n      lemma   gauzaki\n",
    "00002684-n      lemma   objektu\n",
    "\n",
    "\n",
    "3 sarrera ditu: 08687150-n, 09339109-n, 1372009696-n\n",
    "\n",
    "wn.\n",
    "\n",
    "http://adimen.si.ehu.es/cgi-bin/wei/public/wei.consult.perl\n",
    "\n",
    "Ikusi:\n",
    "http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "https://github.com/nltk/nltk_data/issues/43\n",
    "\n",
    "https://codeday.me/es/qa/20190503/633497.html\n",
    "\n",
    "Aupa Kepa! hemen dago dena:\n",
    "\n",
    "http://ixa2.si.ehu.es/ukb/\n",
    "\n",
    "Euskarako Wordnet grafo eta hitzegiak ere hor daude.\n",
    "\n",
    "UKBren kodea hemen:\n",
    "\n",
    "https://github.com/asoroa/ukb\n",
    "\n",
    "Katxarreatzen hasi eta edozein duda bihar han egongo naiz\n",
    "UKB: Graph Base WSD,[53] a collection of programs for performing graph-based Word Sense Disambiguation and lexical similarity/relatedness using a pre-existing Lexical Knowledge Base[54]\n",
    "Ejemplo de uso: Basajaun\n",
    "cd /tartalo03/users/qtleap/pipeline/EU\n",
    "cat proba_fitxategiak/proba4.txt | sh ixa-pipe-pos-eu/run.sh | sh  ixa-pipe-wsd-ukb/run.sh\n",
    "\n",
    "\n",
    "Bert\n",
    "https://github.com/uhh-lt/bert-sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_tokenizer.pt', 'lang': 'eu', 'shorthand': 'eu_bdt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_tagger.pt', 'pretrain_path': '/home/kepa/eu/eu_bdt_models/eu_bdt.pretrain.pt', 'lang': 'eu', 'shorthand': 'eu_bdt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_lemmatizer.pt', 'lang': 'eu', 'shorthand': 'eu_bdt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_parser.pt', 'pretrain_path': '/home/kepa/eu/eu_bdt_models/eu_bdt.pretrain.pt', 'lang': 'eu', 'shorthand': 'eu_bdt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/kepa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /home/kepa/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n",
      "/pytorch/aten/src/ATen/native/LegacyDefinitions.cpp:14: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fruta\tFruta\tNOUN\t_\tCase=Abs|Definite=Def\t0\troot\tedible reproductive body of a seed plant especially one having sweet flesh\tFruta:{'edible_fruit'}\thttps://upload.wikimedia.org/wikipedia/commons/5/5c/Citrus_pulp1.JPG\n",
      "izozkia\tizozki\tNOUN\t_\tAnimacy=Inan|Case=Abs|Definite=Def|Number=Sing\t3\tobj\tice cream or water ice on a small wooden stick\tizozkia:{'popsicle', 'ice_lolly', 'lollipop', 'lolly'}\thttps://upload.wikimedia.org/wikipedia/commons/2/2e/Cucumber%2C_elderflower_and_mint_ice_pop_from_Nicepops_%2818159920902%29.jpg\n",
      "egiteko\tegin\tVERB\t_\tCase=Loc\t4\tadvcl\tagree freely\tegiteko:{'offer', 'volunteer'}\thttps://upload.wikimedia.org/wikipedia/commons/6/62/CI_boardwalk_Sandy_sweepers_jeh.jpg\n",
      "osagaiak\tosagai\tNOUN\t_\tAnimacy=Inan|Case=Abs|Definite=Def|Number=Plur\t1\tnsubj\tsomething determined in relation to something that includes it\tosagaiak:{'part', 'component', 'component_part', 'constituent', 'portion'}\thttp://www.image-net.org/synset?wnid=13809207\n",
      ":\t:\tPUNCT\t_\t_\t1\tpunct\t\t_\t0\n",
      "kilo\tkilo\tNOUN\t_\t_\t20\tnsubj\tone thousand grams; the basic unit of mass adopted under the Systeme International d'Unites\tkilo:{'kilogram', 'kg'}\thttps://upload.wikimedia.org/wikipedia/commons/5/58/HK_Museum_of_History_Steelyard_balance.JPG\n",
      "erdi\terdi\tNOUN\t_\t_\t1\tdet\ta lengthwise dressed half of an animal's carcass used for food\terdi:{'side_of_meat', 'side'}\thttp://www.image-net.org/synset?wnid=7655626\n",
      "marrubi\tmarrubi\tNOUN\t_\tCase=Abs|Definite=Ind\t2\tappos\tEurope\tmarrubi:{'wild_strawberry', 'wood_strawberry', 'fragaria_vesca'}\thttp://www.image-net.org/synset?wnid=12630641\n",
      ",\t,\tPUNCT\t_\t_\t3\tpunct\t\t_\t0\n",
      "edalontzi\tedalontzi\tNOUN\t_\t_\t6\tnmod\ta container for holding liquids while drinking\tedalontzi:{'glass', 'drinking_glass'}\thttps://upload.wikimedia.org/wikipedia/commons/7/73/10_green_bottles.jpeg\n",
      "erdi\terdi\tNOUN\t_\t_\t7\tnmod\t\t_\t0\n",
      "ur\tur\tNOUN\t_\t_\t3\tconj\ta facility that provides a source of water\tur:{'water_system', 'water', 'water_supply'}\thttps://upload.wikimedia.org/wikipedia/commons/2/2e/Concrete_water_pipe.jpg\n",
      ",\t,\tPUNCT\t_\t_\t11\tpunct\t\t_\t0\n",
      "10\t10\tNUM\t_\tNumType=Card\t10\tnummod\t\t_\t0\n",
      "koilarakada\tkoilarakada\tNOUN\t_\t_\t11\tnmod\tas much as a tablespoon will hold\tkoilarakada:{'tablespoon', 'tablespoonful'}\thttps://upload.wikimedia.org/wikipedia/commons/a/a0/Loeffel_03.JPG\n",
      "azukre\tazukre\tNOUN\t_\t_\t7\tconj\ta white crystalline carbohydrate used as a sweetener and preservative\tazukre:{'sugar', 'refined_sugar'}\thttps://upload.wikimedia.org/wikipedia/commons/8/82/Brown_sugar_examples.JPG\n",
      ",\t,\tPUNCT\t_\t_\t14\tpunct\t\t_\t0\n",
      "3\t3\tNUM\t_\tNumType=Card\t14\tnummod\t\t_\t0\n",
      "arrautza\tarrautza\tNOUN\t_\t_\t11\tnmod\t(genetics) the diploid cell resulting from the union of a haploid spermatozoon and ovum (including the organism that develops from that cell)\tarrautza:{'zygote', 'fertilized_ovum'}\thttps://upload.wikimedia.org/wikipedia/commons/6/6c/Psi2.svg\n",
      "zuringo\tzurin\tADJ\t_\t_\t14\tamod\t\t_\t0\n",
      "eta\teta\tCCONJ\t_\t_\t19\tcc\t\t_\t0\n",
      "limoi\tlimoi\tNOUN\t_\t_\t19\tnmod\tyellow oval fruit with juicy acidic flesh\tlimoi:{'lemon'}\thttps://upload.wikimedia.org/wikipedia/commons/8/8d/Citrus_limon_-_Lemon_tree_-_Limonero_-_Limoeiro.JPG\n",
      "baten\tbat\tNUM\t_\tNumType=Card\t17\tnummod\t\t_\t0\n",
      "zukua\tzuku\tNOUN\t_\tAnimacy=Inan|Case=Abs|Definite=Def|Number=Sing\t15\tconj\tthe liquid part that can be extracted from plant or animal tissue by squeezing or cooking\tzukua:{'juice'}\thttps://upload.wikimedia.org/wikipedia/commons/5/5c/53f9da56f0a5c-458x456.png\n",
      ".\t.\tPUNCT\t_\t_\t28\tpunct\t\t_\t0\n",
      "Lehenik\tlehenik\tADV\t_\t_\t25\tadvmod\t\t_\t0\n",
      ",\t,\tPUNCT\t_\t_\t28\tpunct\t\t_\t0\n",
      "marrubiak\tmarrubi\tNOUN\t_\tAnimacy=Inan|Case=Abs|Definite=Def|Number=Plur\t28\tobj\t\t_\t0\n",
      "ondo\tondo\tADV\t_\t_\t25\tadvmod\t\t_\t0\n",
      "garbitu\tgarbitu\tVERB\t_\tVerbForm=Part\t19\tconj\tto wash or wipe with or as if with a mop\tgarbitu:{'mop', 'mop_up', 'wipe_up'}\thttp://www.image-net.org/synset?wnid=1393339\n",
      ",\t,\tPUNCT\t_\t_\t28\tpunct\t\t_\t0\n",
      "zurtoina\tzurtoin\tNOUN\t_\tCase=Abs|Definite=Def|Number=Sing\t28\tobj\t\t_\t0\n",
      "kendu\tkendu\tVERB\t_\tVerbForm=Part\t34\tconj\tobtain by seizing forcibly or violently, also metaphorically\tkendu:{'wrest'}\thttp://www.image-net.org/synset?wnid=1213998\n",
      "eta\teta\tCCONJ\t_\t_\t30\tcc\t\t_\t0\n",
      "zatitu\tzatitu\tVERB\t_\tVerbForm=Part\t28\tconj\tcome apart\tzatitu:{'separate', 'part', 'divide'}\thttp://www.image-net.org/synset?wnid=1557774\n",
      ".\t.\tPUNCT\t_\t_\t34\tpunct\t\t_\t0\n",
      "Ondoren\tondoren\tADV\t_\t_\t30\tadvmod\t\t_\t0\n",
      ",\t,\tPUNCT\t_\t_\t34\tpunct\t\t_\t0\n",
      "nahasi\tnahasi\tVERB\t_\tVerbForm=Part\t0\troot\tovercome as with astonishment or disbelief\tnahasi:{'daze', 'stun', 'bedaze'}\thttps://upload.wikimedia.org/wikipedia/commons/6/63/STUN_Algorithm3.svg\n",
      "beste\tbeste\tDET\t_\t_\t36\tdet\t\t_\t0\n",
      "osagai\tosagai\tNOUN\t_\t_\t40\tnsubj\t\t_\t0\n",
      "guztiak\tguzti\tDET\t_\tCase=Abs|Definite=Def|Number=Plur\t36\tdet\t\t_\t0\n",
      "ontzi\tontzi\tNOUN\t_\t_\t45\tobl\ttableware (eating and serving dishes) collectively\tontzi:{'dishware', 'crockery'}\thttp://www.image-net.org/synset?wnid=3133538\n",
      "batean\tbat\tNUM\t_\tNumType=Card\t38\tnummod\t\t_\t0\n",
      "eta\teta\tCCONJ\t_\t_\t41\tcc\t\t_\t0\n",
      "irabiatu\tirabiatu\tVERB\t_\tVerbForm=Part\t34\tconj\twhip with or as if with a wire whisk\tirabiatu:{'whip', 'whisk'}\thttps://upload.wikimedia.org/wikipedia/commons/a/ab/2904cr_traadvisp.jpg\n",
      ".\t.\tPUNCT\t_\t_\t45\tpunct\t\t_\t0\n",
      "Jarraian\tjarraian\tADV\t_\t_\t45\tadvmod\t\t_\t0\n",
      ",\t,\tPUNCT\t_\t_\t46\tpunct\t\t_\t0\n",
      "gehitu\tgehitu\tVERB\t_\tVerbForm=Part\t40\tconj\tadd up in number or quantity\tgehitu:{'number', 'amount', 'total', 'add_up', 'come'}\thttp://www.image-net.org/synset?wnid=2645007\n",
      "marrubiak\tmarrubi\tNOUN\t_\tAnimacy=Inan|Case=Abs|Definite=Def|Number=Plur\t51\tnmod\t\t_\t0\n",
      "eta\teta\tCCONJ\t_\t_\t48\tcc\t\t_\t0\n",
      "txikitu\ttxikitu\tVERB\t_\tVerbForm=Part\t55\tconj\tdefeat thoroughly\ttxikitu:{'worst', 'whip', 'mop_up', 'pip', 'rack_up'}\thttp://www.image-net.org/synset?wnid=1102997\n",
      "ondo\tondo\tADV\t_\t_\t55\tadvmod\t\t_\t0\n",
      "ondo\tondo\tADV\t_\t_\t55\tadvmod\t\t_\t0\n",
      "irabiagailuaz\tirabiagailu\tNOUN\t_\tCase=Ins|Definite=Def|Number=Sing\t45\tobl\ta mixer incorporating a coil of wires; used for whipping eggs or cream\tirabiagailuaz:{'whisk'}\thttps://upload.wikimedia.org/wikipedia/commons/a/ab/2904cr_traadvisp.jpg\n",
      ".\t.\tPUNCT\t_\t_\t46\tpunct\t\t_\t0\n",
      "Gero\tgero\tADV\t_\t_\t55\tadvmod\t\t_\t0\n",
      ",\t,\tPUNCT\t_\t_\t55\tpunct\t\t_\t0\n",
      "estali\testali\tVERB\t_\tVerbForm=Part\t60\tconj\tcover with a protective sheathing\testali:{'sheathe'}\thttp://www.image-net.org/synset?wnid=1581635\n",
      "film\tfilm\tNOUN\t_\t_\t52\tobl\ta piece of photographic film showing an image with light and shade or colors reversed\tfilm:{'negative'}\thttp://www.image-net.org/synset?wnid=3817191\n",
      "gardenaz\tgarden\tADJ\t_\tCase=Ins|Definite=Def|Number=Sing\t56\tamod\t\t_\t0\n",
      "eta\teta\tCCONJ\t_\t_\t59\tcc\t\t_\t0\n",
      "sartu\tsartu\tVERB\t_\tVerbForm=Part\t56\tconj\tfit snugly into\tsartu:{'insert', 'tuck'}\thttps://upload.wikimedia.org/wikipedia/commons/f/fa/Wikiquote-logo.svg\n",
      "izozkailuan\tizozkailu\tNOUN\t_\tAnimacy=Inan|Case=Ine|Definite=Def|Number=Sing\t65\tobl\telectric refrigerator (trade name Deepfreeze) in which food is frozen and stored for long periods of time\tizozkailuan:{'deep-freeze', 'deepfreeze', 'deep_freezer', 'freezer'}\thttps://upload.wikimedia.org/wikipedia/en/b/b4/Ambox_important.svg\n",
      ".\t.\tPUNCT\t_\t_\t69\tpunct\t\t_\t0\n",
      "Ordu\tordu\tNOUN\t_\t_\t69\tobl\ta suitable moment\tOrdu:{'time'}\thttps://upload.wikimedia.org/wikipedia/commons/e/e0/ChipScaleClock2_HR.jpg\n",
      "eta\teta\tCCONJ\t_\t_\t64\tcc\t\t_\t0\n",
      "erdi\terdi\tADV\t_\t_\t62\tconj\t\t_\t0\n",
      "igaro\tigaro\tVERB\t_\tVerbForm=Part\t59\tconj\tmove past\tigaro:{'go_by', 'travel_by', 'go_past', 'surpass', 'pass', 'pass_by'}\thttps://upload.wikimedia.org/wikipedia/commons/4/49/British_Airways_747-400_World_Traveller_cabin.jpg\n",
      "ondoren\tondoren\tADV\t_\t_\t65\tadvcl\t\t_\t0\n",
      ",\t,\tPUNCT\t_\t_\t72\tpunct\t\t_\t0\n",
      "izozkailutik\tizozkailu\tNOUN\t_\tAnimacy=Inan|Case=Abl|Definite=Def|Number=Sing\t69\tobl\t\t_\t0\n",
      "atera\tatera\tVERB\t_\tVerbForm=Part\t74\tadvcl\tremove (a commodity) from (a supply source)\tatera:{'draw_off', 'draw', 'take_out', 'withdraw'}\thttp://www.image-net.org/synset?wnid=2311387\n",
      "eta\teta\tCCONJ\t_\t_\t72\tcc\t\t_\t0\n",
      "berriro\tberriro\tADV\t_\t_\t72\tadvmod\t\t_\t0\n",
      "irabiatu\tirabiatu\tVERB\t_\tVerbForm=Part\t69\tconj\t\t_\t0\n",
      ".\t.\tPUNCT\t_\t_\t75\tpunct\t\t_\t0\n",
      "Banatu\tbanatu\tVERB\t_\tVerbForm=Part\t75\tadvcl\tto arrange in a systematic order\tBanatu:{'stagger', 'distribute'}\thttps://upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg\n",
      "ontzietan\tontzi\tNOUN\t_\tAnimacy=Inan|Case=Ine|Definite=Def|Number=Plur\t55\tobl\t\t_\t0\n",
      "eta\teta\tCCONJ\t_\t_\t77\tcc\t\t_\t0\n",
      "sartu\tsartu\tVERB\t_\tVerbForm=Part\t69\tconj\t\t_\t0\n",
      "berriro\tberriro\tADV\t_\t_\t77\tadvmod\t\t_\t0\n",
      "izozkailuan\tizozkailu\tNOUN\t_\tAnimacy=Inan|Case=Ine|Definite=Def|Number=Sing\t77\tobl\t\t_\t0\n",
      "ordubetez\tordubete\tNOUN\t_\tAnimacy=Inan|Case=Ins|Definite=Ind\t77\tobl\t\t_\t0\n",
      ".\t.\tPUNCT\t_\t_\t84\tpunct\t\t_\t0\n",
      "Azkenik\tazkenik\tADV\t_\t_\t84\tadvmod\t\t_\t0\n",
      ",\t,\tPUNCT\t_\t_\t84\tpunct\t\t_\t0\n",
      "atera\tatera\tVERB\t_\tVerbForm=Part\t85\tadvcl\t\t_\t0\n",
      "izozkailutik\tizozkailu\tNOUN\t_\tAnimacy=Inan|Case=Abl|Definite=Def|Number=Sing\t77\tobl\t\t_\t0\n",
      "eta\teta\tCCONJ\t_\t_\t87\tcc\t\t_\t0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jan\tjan\tVERB\t_\tVerbForm=Part\t85\tconj\tpass through the esophagus as part of eating or drinking\tjan:{'get_down', 'swallow'}\thttps://upload.wikimedia.org/wikipedia/commons/4/4a/Bahama_Swallow.jpg\n",
      ".\t.\tPUNCT\t_\t_\t69\tpunct\t\t_\t0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "####input file##################\n",
    "input=\"errezeta.txt\" #sys.argv[1]\n",
    "####ouput files#################\n",
    "#estadisticos\n",
    "estadisticaoutput=input+\".out.csv\"\n",
    "\n",
    "#Write all the information in the file\n",
    "estfile = open(estadisticaoutput, \"w\")\n",
    "\n",
    "\n",
    "###############Tratamiento de texto###############################################\n",
    "#quitar todos los retornos \\n si contiene\n",
    "text = open(input).read().replace('\\n', '')\n",
    "#remove text inside parentheses\n",
    "#text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "#separa , . ! ( ) ? ; del texto con espacios, teniendo en cuenta que los no son numeros en el caso de , y . \n",
    "text = re.sub(r\"\\_\", \" \", text)\n",
    "text = re.sub(r\"\\-\", \" \", text)\n",
    "text = re.sub(r'[.]+(?![0-9])', r' . ', text)\n",
    "text = re.sub(r'[,]+(?![0-9])', r' , ', text)\n",
    "text = re.sub(r\"!\", \" ! \", text)\n",
    "text = re.sub(r\"\\(\", \" ( \", text)\n",
    "text = re.sub(r\"\\)\", \" ) \", text)\n",
    "text = re.sub(r\"\\?\", \" ? \", text)\n",
    "text = re.sub(r\";\", \" ; \", text)\n",
    "#sustituye 2 espacios seguidos por 1\n",
    "text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "##############STOPWORDS################################################################################\n",
    "#Generating a word cloud with no optional parameters based on the above string:\n",
    "#from wordcloud import STOPWORDS\n",
    "#stopwords = set(STOPWORDS)\n",
    "#from nltk.corpus import stopwords\n",
    "#stopwords = set(stopwords.words(\"english\"))\n",
    "#words = [w for w in words if not w in stopwords]\n",
    "#stopwords=set()\n",
    "#with open(\"stopwords_formak.txt\") as file:\n",
    "#    for line in file:\n",
    "#        line = line.strip() #preprocess line\n",
    "#        stopwords.add(line)\n",
    "#print (stopwords)\n",
    "stopwords= set(line.strip() for line in open('stopwords_formaketakonektoreak.txt'))\n",
    "stopwords.add(\"gero\")\n",
    "stopwords.add(\"ondo\")\n",
    "stopwords.add(\"ondoren\")\n",
    "stopwords.add(\"ordu\")\n",
    "stopwords.add(\"azkenik\")\n",
    "stopwords.add(\"jarraian\")\n",
    "stopwords.add(\"igaro\")\n",
    "stopwords.add(\"berriro\")\n",
    "stopwords.add(\"lehenik\")\n",
    "#Gehitu konektoreak\n",
    "\n",
    "#stopwords={'to', 'of', 'us'}\n",
    "################WORDCLOUD################################################\n",
    "import matplotlib\n",
    "from wordcloud import WordCloud\n",
    "#Generating a word cloud with no optional parameters based on the above string:\n",
    "# #This is because the wordcloud module ignores stopwords by default. Refer to Part 1 of the NLTK tutorial if the concept of stopwords is new to you.If we wish, we can specify our own set of stopwords, instead of the stopwords provided by default.\n",
    "# #Con relative_scaling = 0, solo se consideran los rangos de las palabras. Si modificamos esto a relative_scaling = 1.0, entonces una palabra que aparece dos veces más frecuentemente aparecerá dos veces el tamaño. Por defecto, relative_scaling = 0.5.\n",
    "# wordcloud = WordCloud(relative_scaling=1.0, stopwords={'to', 'of'}).generate(text)\n",
    "wordcloud = WordCloud(relative_scaling=1.0,stopwords=stopwords).generate(text)\n",
    "#Finally, use matplotlib to render the word cloud:\n",
    "#plot_wordcloud(wordcloud)\n",
    "wordcloudfilename=input+\".png\"\n",
    "#wordcloudfilename=\"resume.png\"\n",
    "wordcloud.to_file(wordcloudfilename)\n",
    "\n",
    "###################PARSER#################################################\n",
    "import stanfordnlp #->/media/datos/Dropbox/ikerkuntza/metrix-env/lib/python3.6/site-packages/stanfordnlp\n",
    "MODELS_DIR = '/home/kepa/eu'\n",
    "#stanfordnlp.download('eu', MODELS_DIR) # Download the English models\n",
    "config = {'processors': 'tokenize,pos,lemma,depparse', # Comma-separated list of processors to use\n",
    "'lang': 'eu', # Language code for the language to build the Pipeline in\n",
    "'tokenize_model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_tokenizer.pt', # Processor-specific arguments are set with keys \"{processor_name}_{argument_name}\"\n",
    "'pos_model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_tagger.pt',\n",
    "'pos_pretrain_path': '/home/kepa/eu/eu_bdt_models/eu_bdt.pretrain.pt',\n",
    "'lemma_model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_lemmatizer.pt',\n",
    "'depparse_model_path': '/home/kepa/eu/eu_bdt_models/eu_bdt_parser.pt',\n",
    "'depparse_pretrain_path': '/home/kepa/eu/eu_bdt_models/eu_bdt.pretrain.pt'\n",
    "}\n",
    "parser = stanfordnlp.Pipeline(**config)\n",
    "##################WORDFRECUENCY###################################################\n",
    "# load a freq file into a dictionary\n",
    "import codecs\n",
    "d = {}\n",
    "#f = codecs.open('compounds.dat', encoding='utf-8')\n",
    "with codecs.open('LB2014Maiztasunak_zenbakiakKenduta.csv',encoding='utf-8') as f:\n",
    "    next(f)\n",
    "    for line in f:\n",
    "       (val,key) = line.split(\",\")\n",
    "       d[key] = val\n",
    "#for key,val in d.items():\n",
    " #   print (key, \"=>\", val)\n",
    "    \n",
    "def zipf_frequency_eu(lemma):\n",
    "    if d.get(lemma):\n",
    "        return (int(d.get(lemma)))\n",
    "    else:\n",
    "        return 1\n",
    "################WORDNET###################################################\n",
    "#Instalar 'wordnet' \n",
    "nltk.download('wordnet')\n",
    "#Add multilingual wordnet\n",
    "nltk.download('omw')\n",
    "from nltk.corpus import wordnet as wn\n",
    "def set_is_empty(some_set):\n",
    "    return some_set == set()\n",
    "###################WSD####################################################\n",
    "#simple_lesk erabiltzeko\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#from pywsd.lesk import simple_lesk\n",
    "from nltk.wsd import lesk\n",
    "##################IMAGES##################################################################\n",
    "import wikipedia\n",
    "\n",
    "#################MAIN#############################################################\n",
    "palabras_raras = []\n",
    "palabras_diferentes = []\n",
    "doc=parser(text)\n",
    "for sent in doc.sentences:\n",
    "\t#Por cada sentencia\n",
    "\tsentence=\"\"\n",
    "\tfor entry in sent.words:\n",
    "\t\tsentence=sentence+' '+entry.text\n",
    "\t#print(sentence)\n",
    "\tfor entry in sent.words:\n",
    "\t\tnueve=\"_\"\n",
    "\t\tdefinicion= ''\n",
    "\t\tsynonyms = []\n",
    "\t\toffset=0\n",
    "\t\t#Por cada palabra\n",
    "\t\t#print(entry.text)\n",
    "\t\tif entry.text.isalpha() and (entry.upos == 'NOUN' or entry.upos == 'VERB') and entry.lemma.lower() not in palabras_diferentes:\n",
    "\t\t\t#Si la palabra es un content word\n",
    "\t\t\tpalabras_diferentes.append(entry.lemma.lower())\n",
    "\t\t\tlemafrequency = zipf_frequency_eu(entry.lemma)\n",
    "\t\t\t#0-6 zaila, 7-34 erdikoak , 35 errazak\n",
    "\t\t\tif lemafrequency <= 35:\n",
    "\t\t\t\t#Si es rara\n",
    "\t\t\t\tpalabras_raras.append(entry.text.lower())\n",
    "\t\t\t\t#Libra sartuz gero, eus/wn-data-eus.tab 3 aukera bilatu:08687150-n, 09339109-n, 13720096-n\n",
    "\t\t\t\t#wn.synsets('dog', pos=wn.VERB)\n",
    "\t\t\t\tsynset_ids = wn.synsets(entry.lemma,lang='eus')\n",
    "\t\t\t\tif entry.upos=='NOUN':\n",
    "\t\t\t\t\tpatron='n'\n",
    "\t\t\t\tif entry.upos=='VERB':\n",
    "\t\t\t\t\tpatron='v'\n",
    "\t\t\t\t#INTENTO DESAMBIGUAR CON LESK            \n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\tanswer2 = lesk(text.split(), entry.text, pos=patron, synsets=synset_ids)\n",
    "\t\t\t\t\tdefinicion=answer2.definition()\n",
    "\t\t\t\t\toffset=\"http://www.image-net.org/synset?wnid=\"+str(answer2.offset())\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\tname=re.sub(r\"\\_\", \" \", answer2.name().split(\".\")[0])\n",
    "\t\t\t\t\t\toffset=wikipedia.page(name).images[0]\n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\tpass                        \n",
    "\t\t\t\t\t#si tiene sinonimos\n",
    "\t\t\t\t\tfor l in answer2.lemma_names():\n",
    "\t\t\t\t\t\tsynonyms.append(l.lower())\n",
    "\t\t\t\t\t\t#print(l.lower())\n",
    "\t\t\t\t\t#print(entry.text.lower())\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\tsynonyms.remove(entry.text.lower())\n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\tpass\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\tsynonyms.remove(entry.lemma)\n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\tpass\n",
    "\t\t\t\t#PRIMER SYNSET FILTRADO POR CATEGORIA\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tcount=0\n",
    "\t\t\t\t\tif entry.upos=='NOUN':\n",
    "\t\t\t\t\t\tpatron='.n.'\n",
    "\t\t\t\t\tif entry.upos=='VERB':\n",
    "\t\t\t\t\t\tpatron='.v.'\n",
    "\t\t\t\t\tfor synset in synset_ids: # [Synset('libra.n.03'), Synset('libra.n.02'), Synset('pound.n.01')]\n",
    "\t\t\t\t\t\tif patron in synset.name() and count==0:\n",
    "\t\t\t\t\t\t\tcount=1\n",
    "\t\t\t\t\t\t\t#print (\"synset.name():\")                 \n",
    "\t\t\t\t\t\t\t#print (synset.name())   #pound.n.01\n",
    "\t\t\t\t\t\t\t#sinonimoak bilatu\n",
    "\t\t\t\t\t\t\t#print (str(synset.lemmas(lang='eus')))\n",
    "\t\t\t\t\t\t\tfor l in synset.lemmas(lang='eus'): #[Lemma('pound.n.01.libera'), Lemma('pound.n.01.libra')]\n",
    "\t\t\t\t\t\t\t\tsynonyms.append(l.name())\n",
    "\t\t\t\t\t\t\t\t#print (\"l.name():\") #libera, libra              \n",
    "\t\t\t\t\t\t\t\t#print (l.name())\n",
    "\t\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\tsynonyms.remove(entry.text)\n",
    "\t\t\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\t\t\tpass\n",
    "\t\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\tsynonyms.remove(entry.lemma)\n",
    "\t\t\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\t\t\tpass\n",
    "\t\t\t\t\t\t\tdefinicion=synset.definition()\n",
    "\t\t\t\t\t\t\toffset=\"http://www.image-net.org/synset?wnid=\"+str(synset.offset())\n",
    "\t\t\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\t\t\tname=re.sub(r\"\\_\", \" \", synset.name().split(\".\")[0])\n",
    "\t\t\t\t\t\t\t\toffset=wikipedia.page(name).images[0]\n",
    "\t\t\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\t\t\tpass\n",
    "\t\t#print(\"palabras_raras:\")\n",
    "\t\t#print(palabras_raras)\n",
    "\t\tif not set_is_empty(set(synonyms)):\n",
    "\t\t\tnueve=entry.text+\":\"+str(set(synonyms))\n",
    "\t\telse:\n",
    "\t\t\tnueve=\"_\"\n",
    "\t\tprint(entry.text+\"\\t\"+entry.lemma+\"\\t\"+entry.upos+\"\\t\"+entry.xpos+\"\\t\"+entry.feats+\"\\t\"+str(entry.governor)+\"\\t\"+str(entry.dependency_relation)+\"\\t\"+definicion+\"\\t\"+nueve+\"\\t\"+str(offset))\n",
    "\t\testfile.write(\"%s\" % entry.text+\"\\t\"+entry.lemma+\"\\t\"+entry.upos+\"\\t\"+entry.xpos+\"\\t\"+entry.feats+\"\\t\"+str(entry.governor)+\"\\t\"+str(entry.dependency_relation[:2])+\"\\t\"+definicion+\"\\t\"+nueve+\"\\t\"+str(offset))\n",
    "\t\testfile.write(\"\\n\")\n",
    "estfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
